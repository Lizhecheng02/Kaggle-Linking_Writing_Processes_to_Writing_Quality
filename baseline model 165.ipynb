{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["import polars as pl\n","import pandas as pd\n","import numpy as np\n","import re\n","import os\n","from sklearn import metrics, model_selection\n","from sklearn.preprocessing import StandardScaler\n","import lightgbm as lgb\n","import xgboost as xgb\n","import catboost as cb\n","import optuna\n","from scipy.stats import skew, kurtosis\n","import warnings\n","from tqdm import tqdm\n","import json\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"trusted":true},"outputs":[],"source":["class CFG:\n","    is_train_lgbm_model = False\n","    is_train_lgbm_optuna = True\n","    is_train_xgb_model = False\n","    is_train_xgb_optuna = True\n","    is_train_cb_model = False\n","    is_train_cb_optuna = True"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["num_cols = ['down_time', 'up_time', 'action_time',\n","            'cursor_position', 'word_count']\n","activities = ['Input', 'Remove/Cut', 'Nonproduction', 'Replace', 'Paste']\n","events = ['q', 'Space', 'Backspace', 'Shift', 'ArrowRight', 'Leftclick', 'ArrowLeft',\n","          '.', ',', 'ArrowDown', 'ArrowUp', 'Enter', 'CapsLock', \"'\", 'Delete', 'Unidentified']\n","text_changes = ['q', ' ', '.', ',', '\\n', \"'\",\n","                '\"', '-', '?', ';', '=', '/', '\\\\', ':']"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def count_by_values(df, colname, values):\n","    fts = df.select(pl.col('id').unique(maintain_order=True))\n","    for i, value in enumerate(values):\n","        tmp_df = df.group_by('id').agg(pl.col(colname).is_in(\n","            [value]).sum().alias(f'{colname}_{i}_cnt'))\n","        fts = fts.join(tmp_df, on='id', how='left')\n","    return fts\n","\n","\n","def dev_feats(df):\n","    print(\"< Count by values features >\")\n","\n","    feats = count_by_values(df, 'activity', activities)\n","    feats = feats.join(count_by_values(df, 'text_change',\n","                       text_changes), on='id', how='left')\n","    feats = feats.join(count_by_values(\n","        df, 'down_event', events), on='id', how='left')\n","    feats = feats.join(count_by_values(\n","        df, 'up_event', events), on='id', how='left')\n","\n","    print(\"< Input words stats features >\")\n","\n","    temp = df.filter((~pl.col('text_change').str.contains('=>'))\n","                     & (pl.col('text_change') != 'NoChange'))\n","    temp = temp.group_by('id').agg(\n","        pl.col('text_change').str.concat('').str.extract_all(r'q+'))\n","    temp = temp.with_columns(\n","        input_word_count=pl.col('text_change').list.lengths(),\n","        input_word_length_mean=pl.col('text_change').apply(\n","            lambda x: np.mean([len(i) for i in x] if len(x) > 0 else 0)\n","        ),\n","        input_word_length_max=pl.col('text_change').apply(\n","            lambda x: np.max([len(i) for i in x] if len(x) > 0 else 0)\n","        ),\n","        input_word_length_std=pl.col('text_change').apply(\n","            lambda x: np.std([len(i) for i in x] if len(x) > 0 else 0)\n","        ),\n","        input_word_length_median=pl.col('text_change').apply(\n","            lambda x: np.median([len(i) for i in x] if len(x) > 0 else 0)\n","        ),\n","        input_word_length_skew=pl.col('text_change').apply(\n","            lambda x: skew([len(i) for i in x] if len(x) > 0 else 0)\n","        )\n","    )\n","    temp = temp.drop('text_change')\n","    feats = feats.join(temp, on='id', how='left')\n","\n","    print(\"< Numerical columns features >\")\n","\n","    temp = df.group_by(\"id\").agg(\n","        pl.sum('action_time').suffix('_sum'),\n","        pl.mean(num_cols).suffix('_mean'),\n","        pl.std(num_cols).suffix('_std'),\n","        pl.median(num_cols).suffix('_median'),\n","        pl.min(num_cols).suffix('_min'),\n","        pl.max(num_cols).suffix('_max'),\n","        pl.quantile(num_cols, 0.5).suffix('_quantile'))\n","    feats = feats.join(temp, on='id', how='left')\n","\n","    print(\"< Categorical columns features >\")\n","\n","    temp = df.group_by(\"id\").agg(\n","        pl.n_unique(['activity', 'down_event', 'up_event', 'text_change'])\n","    )\n","    feats = feats.join(temp, on='id', how='left')\n","\n","    print(\"< Idle time features >\")\n","\n","    temp = df.with_columns(pl.col('up_time').shift().over(\n","        'id').alias('up_time_lagged'))\n","    temp = temp.with_columns((abs(pl.col(\n","        'down_time') - pl.col('up_time_lagged')) / 1000).fill_null(0).alias('time_diff'))\n","    temp = temp.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n","    temp = temp.group_by(\"id\").agg(\n","        inter_key_largest_lantency=pl.max('time_diff'),\n","        inter_key_median_lantency=pl.median('time_diff'),\n","        mean_pause_time=pl.mean('time_diff'),\n","        std_pause_time=pl.std('time_diff'),\n","        total_pause_time=pl.sum('time_diff'),\n","        pauses_half_sec=pl.col('time_diff').filter(\n","            (pl.col('time_diff') > 0.5) & (pl.col('time_diff') < 1)\n","        ).count(),\n","        pauses_1_sec=pl.col('time_diff').filter(\n","            (pl.col('time_diff') > 1) & (pl.col('time_diff') < 1.5)\n","        ).count(),\n","        pauses_1_half_sec=pl.col('time_diff').filter(\n","            (pl.col('time_diff') > 1.5) & (pl.col('time_diff') < 2)\n","        ).count(),\n","        pauses_2_sec=pl.col('time_diff').filter(\n","            (pl.col('time_diff') > 2) & (pl.col('time_diff') < 3)\n","        ).count(),\n","        pauses_3_sec=pl.col('time_diff').filter(\n","            pl.col('time_diff') > 3).count()\n","    )\n","    feats = feats.join(temp, on='id', how='left')\n","\n","    print(\"< P-bursts features >\")\n","\n","    temp = df.with_columns(pl.col('up_time').shift().over(\n","        'id').alias('up_time_lagged'))\n","    temp = temp.with_columns((abs(pl.col(\n","        'down_time') - pl.col('up_time_lagged')) / 1000).fill_null(0).alias('time_diff'))\n","    temp = temp.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n","    temp = temp.with_columns(pl.col('time_diff') < 2)\n","    temp = temp.with_columns(pl.when(pl.col(\"time_diff\") & pl.col(\"time_diff\").is_last(\n","    )).then(pl.count()).over(pl.col(\"time_diff\").rle_id()).alias('P-bursts'))\n","    temp = temp.drop_nulls()\n","    temp = temp.group_by(\"id\").agg(\n","        pl.mean('P-bursts').suffix('_mean'),\n","        pl.std('P-bursts').suffix('_std'),\n","        pl.count('P-bursts').suffix('_count'),\n","        pl.median('P-bursts').suffix('_median'),\n","        pl.max('P-bursts').suffix('_max'),\n","        pl.first('P-bursts').suffix('_first'),\n","        pl.last('P-bursts').suffix('_last')\n","    )\n","    feats = feats.join(temp, on='id', how='left')\n","\n","    print(\"< R-bursts features >\")\n","\n","    temp = df.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n","    temp = temp.with_columns(pl.col('activity').is_in(['Remove/Cut']))\n","    temp = temp.with_columns(pl.when(pl.col(\"activity\") & pl.col(\"activity\").is_last(\n","    )).then(pl.count()).over(pl.col(\"activity\").rle_id()).alias('R-bursts'))\n","    temp = temp.drop_nulls()\n","    temp = temp.group_by(\"id\").agg(\n","        pl.mean('R-bursts').suffix('_mean'),\n","        pl.std('R-bursts').suffix('_std'),\n","        pl.median('R-bursts').suffix('_median'),\n","        pl.max('R-bursts').suffix('_max'),\n","        pl.first('R-bursts').suffix('_first'),\n","        pl.last('R-bursts').suffix('_last')\n","    )\n","    feats = feats.join(temp, on='id', how='left')\n","\n","    return feats"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def q1(x):\n","    return x.quantile(0.25)\n","\n","\n","def q3(x):\n","    return x.quantile(0.75)\n","\n","\n","AGGREGATIONS = ['count', 'mean', 'min', 'max', 'first', 'last', q1, 'median', q3, 'sum']\n","\n","\n","def reconstruct_essay(currTextInput):\n","    essayText = \"\"\n","    for Input in currTextInput.values:\n","        if Input[0] == 'Replace':\n","            replaceTxt = Input[2].split(' => ')\n","            essayText = essayText[:Input[1] - len(replaceTxt[1])] + replaceTxt[1] + \\\n","                essayText[Input[1] - len(replaceTxt[1]) + len(replaceTxt[0]):]\n","            continue\n","        if Input[0] == 'Paste':\n","            essayText = essayText[:Input[1] - len(Input[2])] + \\\n","                Input[2] + essayText[Input[1] - len(Input[2]):]\n","            continue\n","        if Input[0] == 'Remove/Cut':\n","            essayText = essayText[:Input[1]] + \\\n","                essayText[Input[1] + len(Input[2]):]\n","            continue\n","        if \"M\" in Input[0]:\n","            croppedTxt = Input[0][10:]\n","            splitTxt = croppedTxt.split(' To ')\n","            valueArr = [item.split(', ') for item in splitTxt]\n","            moveData = (int(valueArr[0][0][1:]), int(\n","                valueArr[0][1][:-1]), int(valueArr[1][0][1:]), int(valueArr[1][1][:-1]))\n","            if moveData[0] != moveData[2]:\n","                if moveData[0] < moveData[2]:\n","                    essayText = essayText[:moveData[0]] + \\\n","                        essayText[moveData[1]:moveData[3]] + \\\n","                        essayText[moveData[0]:moveData[1]] + \\\n","                        essayText[moveData[3]:]\n","                else:\n","                    essayText = essayText[:moveData[2]] + \\\n","                        essayText[moveData[0]:moveData[1]] + \\\n","                        essayText[moveData[2]:moveData[0]] + \\\n","                        essayText[moveData[1]:]\n","            continue\n","        essayText = essayText[:Input[1] - len(Input[2])] + \\\n","            Input[2] + essayText[Input[1] - len(Input[2]):]\n","    return essayText\n","\n","\n","def get_essay_df(df):\n","    df = df[df.activity != 'Nonproduction']\n","    temp = df.groupby('id').apply(lambda x: reconstruct_essay(\n","        x[['activity', 'cursor_position', 'text_change']]))\n","    essay_df = pd.DataFrame({'id': df['id'].unique().tolist()})\n","    essay_df = essay_df.merge(temp.rename('essay'), on='id')\n","    return essay_df\n","\n","\n","def word_feats(df):\n","    essay_df = df\n","    df['word'] = df['essay'].apply(lambda x: re.split(' |\\\\n|\\\\.|\\\\?|\\\\!', x))\n","    df = df.explode('word')\n","    df['word_len'] = df['word'].apply(lambda x: len(x))\n","    df = df[df['word_len'] != 0]\n","\n","    word_agg_df = df[['id', 'word_len']].groupby(['id']).agg(AGGREGATIONS)\n","    word_agg_df.columns = ['_'.join(x) for x in word_agg_df.columns]\n","    word_agg_df['id'] = word_agg_df.index\n","    word_agg_df = word_agg_df.reset_index(drop=True)\n","    return word_agg_df\n","\n","\n","def sent_feats(df):\n","    df['sent'] = df['essay'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!', x))\n","    df = df.explode('sent')\n","    df['sent'] = df['sent'].apply(lambda x: x.replace('\\n', '').strip())\n","    # Number of characters in sentences\n","    df['sent_len'] = df['sent'].apply(lambda x: len(x))\n","    # Number of words in sentences\n","    df['sent_word_count'] = df['sent'].apply(lambda x: len(x.split(' ')))\n","    df = df[df.sent_len != 0].reset_index(drop=True)\n","\n","    sent_agg_df = pd.concat([df[['id', 'sent_len']].groupby(['id']).agg(AGGREGATIONS),\n","                             df[['id', 'sent_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1)\n","    sent_agg_df.columns = ['_'.join(x) for x in sent_agg_df.columns]\n","    sent_agg_df['id'] = sent_agg_df.index\n","    sent_agg_df = sent_agg_df.reset_index(drop=True)\n","    sent_agg_df.drop(columns=[\"sent_word_count_count\"], inplace=True)\n","    sent_agg_df = sent_agg_df.rename(columns={\"sent_len_count\": \"sent_count\"})\n","    return sent_agg_df\n","\n","\n","def parag_feats(df):\n","    df['paragraph'] = df['essay'].apply(lambda x: x.split('\\n'))\n","    df = df.explode('paragraph')\n","    # Number of characters in paragraphs\n","    df['paragraph_len'] = df['paragraph'].apply(lambda x: len(x))\n","    # Number of words in paragraphs\n","    df['paragraph_word_count'] = df['paragraph'].apply(\n","        lambda x: len(x.split(' ')))\n","    df = df[df.paragraph_len != 0].reset_index(drop=True)\n","\n","    paragraph_agg_df = pd.concat([df[['id', 'paragraph_len']].groupby(['id']).agg(AGGREGATIONS),\n","                                  df[['id', 'paragraph_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1)\n","    paragraph_agg_df.columns = ['_'.join(x) for x in paragraph_agg_df.columns]\n","    paragraph_agg_df['id'] = paragraph_agg_df.index\n","    paragraph_agg_df = paragraph_agg_df.reset_index(drop=True)\n","    paragraph_agg_df.drop(columns=[\"paragraph_word_count_count\"], inplace=True)\n","    paragraph_agg_df = paragraph_agg_df.rename(\n","        columns={\"paragraph_len_count\": \"paragraph_count\"})\n","    return paragraph_agg_df\n","\n","\n","def product_to_keys(logs, essays):\n","    essays['product_len'] = essays.essay.str.len()\n","    tmp_df = logs[logs.activity.isin(['Input', 'Remove/Cut'])].groupby(['id']).agg(\n","        {'activity': 'count'}).reset_index().rename(columns={'activity': 'keys_pressed'})\n","    essays = essays.merge(tmp_df, on='id', how='left')\n","    essays['product_to_keys'] = essays['product_len'] / essays['keys_pressed']\n","    return essays[['id', 'product_to_keys']]\n","\n","\n","def get_keys_pressed_per_second(logs):\n","    temp_df = logs[logs['activity'].isin(['Input', 'Remove/Cut'])].groupby(\n","        ['id']).agg(keys_pressed=('event_id', 'count')).reset_index()\n","    temp_df_2 = logs.groupby(['id']).agg(min_down_time=(\n","        'down_time', 'min'), max_up_time=('up_time', 'max')).reset_index()\n","    temp_df = temp_df.merge(temp_df_2, on='id', how='left')\n","    temp_df['keys_per_second'] = temp_df['keys_pressed'] / \\\n","        ((temp_df['max_up_time'] - temp_df['min_down_time']) / 1000)\n","    return temp_df[['id', 'keys_per_second']]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["data_path = './'\n","train_logs = pl.scan_csv(data_path + 'train_logs.csv')\n","train_feats = dev_feats(train_logs)\n","train_feats = train_feats.collect().to_pandas()\n","\n","print('< Essay Reconstruction >')\n","train_logs = train_logs.collect().to_pandas()\n","train_essays = get_essay_df(train_logs)\n","train_feats = train_feats.merge(word_feats(train_essays), on='id', how='left')\n","train_feats = train_feats.merge(sent_feats(train_essays), on='id', how='left')\n","train_feats = train_feats.merge(parag_feats(train_essays), on='id', how='left')\n","train_feats = train_feats.merge(\n","    get_keys_pressed_per_second(train_logs), on='id', how='left')\n","train_feats = train_feats.merge(product_to_keys(\n","    train_logs, train_essays), on='id', how='left')\n","# train_feats = train_feats.fillna(0.0)\n","\n","print('< Mapping >')\n","train_scores = pd.read_csv(data_path + 'train_scores.csv')\n","data = train_feats.merge(train_scores, on='id', how='left')\n","x = data.drop(['id', 'score'], axis=1)\n","y = data['score'].values\n","print(f'Number of features: {len(x.columns)}')\n","\n","\n","print('< Testing Data >')\n","test_logs = pl.scan_csv(data_path + 'test_logs.csv')\n","test_feats = dev_feats(test_logs)\n","test_feats = test_feats.collect().to_pandas()\n","\n","test_logs = test_logs.collect().to_pandas()\n","test_essays = get_essay_df(test_logs)\n","test_feats = test_feats.merge(word_feats(test_essays), on='id', how='left')\n","test_feats = test_feats.merge(sent_feats(test_essays), on='id', how='left')\n","test_feats = test_feats.merge(parag_feats(test_essays), on='id', how='left')\n","test_feats = test_feats.merge(\n","    get_keys_pressed_per_second(test_logs), on='id', how='left')\n","test_feats = test_feats.merge(product_to_keys(\n","    test_logs, test_essays), on='id', how='left')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["target_col = ['score']\n","drop_cols = ['id']\n","train_cols = [\n","    col for col in train_feats.columns if col not in target_col + drop_cols\n","]\n","\n","print('< Learning and Evaluation >')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def train_lgbm_model(train_feats, test_feats):\n","    print(\"Training LightGBM Model ..........\")\n","    TEST_PREDS = np.zeros((len(test_feats), 1))\n","\n","    os.makedirs('./baseline_lgb_models', exist_ok=True)\n","\n","    EPOCHS = 1\n","    SPLIT = 10\n","\n","    test_prediction_list = []\n","    model_dict = {}\n","    scores = []\n","    preds = np.zeros((len(train_feats), 1))\n","\n","    best_params = {\n","        'reg_alpha': 0.6016917340618352,\n","        'reg_lambda': 3.8071290717767194,\n","        'colsample_bytree': 0.45216556596658897,\n","        'subsample': 0.4832292138435902,\n","        'learning_rate': 0.001,\n","        'num_leaves': 11,\n","        'max_depth': 27,\n","        'min_child_samples': 17,\n","        'n_jobs': 4\n","    }\n","\n","    for i in range(EPOCHS):\n","        kf = model_selection.KFold(\n","            n_splits=SPLIT, random_state=42 + i * 10, shuffle=True)\n","        valid_preds = np.zeros(train_feats.shape[0])\n","        X_test = test_feats[train_cols]\n","\n","        for fold, (train_idx, valid_idx) in enumerate(kf.split(train_feats)):\n","            print(f'Epoch: {i + 1} Fold: {fold + 1}')\n","            X_train, y_train = train_feats.iloc[train_idx][train_cols], train_feats.iloc[train_idx][target_col]\n","            X_valid, y_valid = train_feats.iloc[valid_idx][train_cols], train_feats.iloc[valid_idx][target_col]\n","            params = {\n","                \"objective\": \"regression\",\n","                \"metric\": \"rmse\",\n","                \"random_state\": 42,\n","                \"n_estimators\": 11_861,\n","                \"verbosity\": 1,\n","                **best_params\n","            }\n","            model = lgb.LGBMRegressor(**params)\n","            early_stopping_callback = lgb.early_stopping(\n","                100, first_metric_only=True, verbose=True\n","            )\n","\n","            model.fit(\n","                X_train, y_train,\n","                eval_set=[(X_valid, y_valid)],\n","                callbacks=[early_stopping_callback]\n","            )\n","\n","            valid_predict = model.predict(X_valid)\n","            valid_preds[valid_idx] = valid_predict\n","            preds[valid_idx, 0] += valid_predict / EPOCHS\n","\n","            test_predict = model.predict(X_test)\n","            TEST_PREDS[:, 0] += test_predict / EPOCHS / SPLIT\n","            test_prediction_list.append(test_predict)\n","\n","            score = metrics.mean_squared_error(\n","                y_valid, valid_predict, squared=False)\n","            model_dict[f'Epoch{i + 1}-Fold{fold + 1}'] = model\n","            model.booster_.save_model(\n","                f'./baseline_lgb_models/lgbm_model_epoch{i + 1}_fold{fold + 1}.txt')\n","\n","        final_score = metrics.mean_squared_error(\n","            train_feats[target_col], valid_preds, squared=False)\n","        scores.append(final_score)\n","\n","    print(\"Avg Loss:\", np.mean(scores))\n","\n","    print('metric LGBM = {:.5f}'.format(metrics.mean_squared_error(\n","        train_feats[target_col], preds[:, 0], squared=False)))\n","\n","    return TEST_PREDS"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def train_xgb_model(train_feats, test_feats):\n","    print(\"Training XGBoost Model ..........\")\n","    TEST_PREDS = np.zeros((len(test_feats), 1))\n","\n","    os.makedirs('./baseline_xgb_models', exist_ok=True)\n","\n","    EPOCHS = 1\n","    SPLIT = 10\n","\n","    test_prediction_list = []\n","    model_dict = {}\n","    scores = []\n","    preds = np.zeros((len(train_feats), 1))\n","\n","    best_params = {\n","        'reg_alpha': 0.9755323106008543,\n","        'reg_lambda': 4.196929779592309,\n","        'colsample_bytree': 0.8091449108500868,\n","        'subsample': 0.4861049766563444,\n","        'learning_rate': 0.001,\n","        'max_depth': 14,\n","        'min_child_weight': 4.275719603876376,\n","        'gamma': 1.659538839916536,\n","        'n_jobs': 4\n","    }\n","\n","    for i in range(EPOCHS):\n","        kf = model_selection.KFold(\n","            n_splits=SPLIT, random_state=42 + i * 10, shuffle=True)\n","        valid_preds = np.zeros(train_feats.shape[0])\n","        X_test = test_feats[train_cols]\n","\n","        for fold, (train_idx, valid_idx) in enumerate(kf.split(train_feats)):\n","            print(f'Epoch: {i + 1} Fold: {fold + 1}')\n","            X_train, y_train = train_feats.iloc[train_idx][train_cols], train_feats.iloc[train_idx][target_col]\n","            X_valid, y_valid = train_feats.iloc[valid_idx][train_cols], train_feats.iloc[valid_idx][target_col]\n","            params = {\n","                \"objective\": \"reg:squarederror\",\n","                \"eval_metric\": \"rmse\",\n","                \"random_state\": 42,\n","                \"n_estimators\": 12358,\n","                \"verbosity\": 0,\n","                **best_params\n","            }\n","            model = xgb.XGBRegressor(**params)\n","\n","            model.fit(\n","                X_train, y_train,\n","                eval_set=[(X_valid, y_valid)],\n","                early_stopping_rounds=100,\n","                verbose=False\n","            )\n","\n","            valid_predict = model.predict(X_valid)\n","            valid_preds[valid_idx] = valid_predict\n","            preds[valid_idx, 0] += valid_predict / EPOCHS\n","\n","            test_predict = model.predict(X_test)\n","            TEST_PREDS[:, 0] += test_predict / EPOCHS / SPLIT\n","            test_prediction_list.append(test_predict)\n","\n","            score = metrics.mean_squared_error(\n","                y_valid, valid_predict, squared=False)\n","            model_dict[f'Epoch{i + 1}-Fold{fold + 1}'] = model\n","            model.save_model(\n","                f'./baseline_xgb_models/xgb_model_epoch{i + 1}_fold{fold + 1}.json')\n","            # model.load_model(f'./baseline_xgb_models/xgb_model_epoch{i + 1}_fold{fold + 1}.json')\n","\n","        final_score = metrics.mean_squared_error(\n","            train_feats[target_col], valid_preds, squared=False)\n","        scores.append(final_score)\n","\n","    print(\"Avg Loss:\", np.mean(scores))\n","\n","    print('metric XGB = {:.5f}'.format(metrics.mean_squared_error(\n","        train_feats[target_col], preds[:, 0], squared=False)))\n","\n","    return TEST_PREDS"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def train_cb_model(train_feats, test_feats):\n","    print(\"Training CatBoost Model ..........\")\n","    TEST_PREDS = np.zeros((len(test_feats), 1))\n","\n","    os.makedirs('./baseline_cb_models', exist_ok=True)\n","\n","    EPOCHS = 1\n","    SPLIT = 10\n","\n","    test_prediction_list = []\n","    model_dict = {}\n","    scores = []\n","    preds = np.zeros((len(train_feats), 1))\n","\n","    best_params = {\n","        'l2_leaf_reg': 6.917257836039084,\n","        'colsample_bylevel': 0.8282277872952677,\n","        'subsample': 0.8138165885029924,\n","        'learning_rate': 0.02674136303965473,\n","        'depth': 6,\n","        'thread_count': 4,\n","        'min_child_samples': 4\n","    }\n","\n","    for i in range(EPOCHS):\n","        kf = model_selection.KFold(\n","            n_splits=SPLIT, random_state=42 + i * 10, shuffle=True)\n","        valid_preds = np.zeros(train_feats.shape[0])\n","        X_test = test_feats[train_cols]\n","\n","        for fold, (train_idx, valid_idx) in enumerate(kf.split(train_feats)):\n","            print(f'Epoch: {i + 1} Fold: {fold + 1}')\n","            X_train, y_train = train_feats.iloc[train_idx][train_cols], train_feats.iloc[train_idx][target_col]\n","            X_valid, y_valid = train_feats.iloc[valid_idx][train_cols], train_feats.iloc[valid_idx][target_col]\n","\n","            model = cb.CatBoostRegressor(\n","                iterations=5801,\n","                loss_function='RMSE',\n","                random_seed=2023,\n","                verbose=False,\n","                **best_params\n","            )\n","\n","            model.fit(\n","                X_train, y_train,\n","                eval_set=[(X_valid, y_valid)],\n","                early_stopping_rounds=100,\n","                verbose=True\n","            )\n","\n","            valid_predict = model.predict(X_valid)\n","            valid_preds[valid_idx] = valid_predict\n","            preds[valid_idx, 0] += valid_predict / EPOCHS\n","\n","            test_predict = model.predict(X_test)\n","            TEST_PREDS[:, 0] += test_predict / EPOCHS / SPLIT\n","            test_prediction_list.append(test_predict)\n","\n","            score = metrics.mean_squared_error(\n","                y_valid, valid_predict, squared=False)\n","            model_dict[f'Epoch{i + 1}-Fold{fold + 1}'] = model\n","            model.save_model(\n","                f'./baseline_cb_models/cb_model_epoch{i + 1}_fold{fold + 1}.cbm')\n","            # model.load_model(f'./baseline_cb_models/cb_model_epoch{i + 1}_fold{fold + 1}.cbm')\n","\n","        final_score = metrics.mean_squared_error(\n","            train_feats[target_col], valid_preds, squared=False)\n","        scores.append(final_score)\n","\n","    print(\"Avg Loss:\", np.mean(scores))\n","\n","    print('metric CB = {:.5f}'.format(metrics.mean_squared_error(\n","        train_feats[target_col], preds[:, 0], squared=False)))\n","\n","    return TEST_PREDS"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"trusted":true},"outputs":[],"source":["def train_lgbm_optuna(train_feats):\n","    os.makedirs('./baseline_lgb_models_optuna', exist_ok=True)\n","\n","    def objective(trial):\n","        EPOCHS = 2\n","        SPLIT = 10\n","\n","        model_dict = {}\n","        scores = []\n","        preds = np.zeros((len(train_feats), 1))\n","\n","        best_params = {\n","            'reg_alpha': trial.suggest_float(\"reg_alpha\", 0.0, 1.0),\n","            'reg_lambda': trial.suggest_float(\"reg_lambda\", 0.0, 5.0),\n","            'colsample_bytree': trial.suggest_float(\"colsample_bytree\", 0.4, 1.0),\n","            'subsample': trial.suggest_float(\"subsample\", 0.4, 1.0),\n","            'learning_rate': trial.suggest_float(\"subsample\", 0.001, 1.0),\n","            'num_leaves': trial.suggest_int(\"num_leaves\", 5, 50),\n","            'max_depth': trial.suggest_int(\"max_depth\", 5, 30),\n","            'min_child_samples': trial.suggest_int(\"min_child_samples\", 2, 30),\n","            'n_jobs': 4,\n","            \"n_estimators\": trial.suggest_int(\"n_estimators\", 1000, 20000)\n","        }\n","\n","        for i in range(EPOCHS):\n","            kf = model_selection.KFold(\n","                n_splits=SPLIT, random_state=42 + i * 10, shuffle=True)\n","            valid_preds = np.zeros(train_feats.shape[0])\n","\n","            for fold, (train_idx, valid_idx) in enumerate(kf.split(train_feats)):\n","                print(f'Epoch: {i + 1} Fold: {fold + 1}')\n","                X_train, y_train = train_feats.iloc[train_idx][train_cols], train_feats.iloc[train_idx][target_col]\n","                X_valid, y_valid = train_feats.iloc[valid_idx][train_cols], train_feats.iloc[valid_idx][target_col]\n","                params = {\n","                    \"objective\": \"regression\",\n","                    \"metric\": \"rmse\",\n","                    \"random_state\": 42,\n","                    \"verbosity\": 1,\n","                    **best_params\n","                }\n","                model = lgb.LGBMRegressor(**params)\n","                early_stopping_callback = lgb.early_stopping(\n","                    100, first_metric_only=True, verbose=True\n","                )\n","\n","                model.fit(\n","                    X_train, y_train,\n","                    eval_set=[(X_valid, y_valid)],\n","                    callbacks=[early_stopping_callback]\n","                )\n","\n","                valid_predict = model.predict(X_valid)\n","                valid_preds[valid_idx] = valid_predict\n","                preds[valid_idx, 0] += valid_predict / EPOCHS\n","\n","                score = metrics.mean_squared_error(\n","                    y_valid, valid_predict, squared=False)\n","                model_dict[f'Epoch{i + 1}-Fold{fold + 1}'] = model\n","                model.booster_.save_model(\n","                    f'./baseline_lgb_models_optuna/lgbm_model_epoch{i + 1}_fold{fold + 1}.txt')\n","\n","            final_score = metrics.mean_squared_error(\n","                train_feats[target_col], valid_preds, squared=False)\n","            scores.append(final_score)\n","\n","        print(\"Avg Loss:\", np.mean(scores))\n","        return np.mean(scores)\n","\n","    study = optuna.create_study(direction=\"minimize\")\n","    study.optimize(objective, n_trials=200)\n","\n","    print(\"LightGBM Best trial:\")\n","    trial = study.best_trial\n","    print(f\"Value: {trial.value}\")\n","    print(\"Params: \")\n","    for key, value in trial.params.items():\n","        print(f\"{key}: {value}\")\n","    \n","    with open('lgbm_best_params.json', 'w') as json_file:\n","        json.dump(trial.params, json_file, indent=4)\n","\n","    print(\"Save LightGBM best_params to json file\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def train_xgb_optuna(train_feats):\n","    os.makedirs('./baseline_xgb_models_optuna', exist_ok=True)\n","\n","    def objective(trial):\n","        EPOCHS = 2\n","        SPLIT = 10\n","\n","        model_dict = {}\n","        scores = []\n","        preds = np.zeros((len(train_feats), 1))\n","\n","        best_params = {\n","            'reg_alpha': trial.suggest_float(\"reg_alpha\", 0.0, 1.0),\n","            'reg_lambda': trial.suggest_float(\"reg_lambda\", 0.0, 5.0),\n","            'colsample_bytree': trial.suggest_float(\"colsample_bytree\", 0.4, 1.0),\n","            'subsample': trial.suggest_float(\"subsample\", 0.4, 1.0),\n","            'learning_rate': trial.suggest_float(\"subsample\", 0.001, 1.0),\n","            'max_depth': trial.suggest_int(\"max_depth\", 5, 30),\n","            'min_child_weight': trial.suggest_float(\"min_child_weight\", 1.0, 5.0),\n","            'gamma': trial.suggest_float(\"gamma\", 0.0, 10.0),\n","            'max_delta_step': trial.suggest_int(\"max_delta_step\", 1, 5),\n","            'n_jobs': 4,\n","            \"n_estimators\": trial.suggest_int(\"n_estimators\", 1000, 20000)\n","        }\n","\n","        for i in range(EPOCHS):\n","            kf = model_selection.KFold(\n","                n_splits=SPLIT, random_state=42 + i * 10, shuffle=True)\n","            valid_preds = np.zeros(train_feats.shape[0])\n","\n","            for fold, (train_idx, valid_idx) in enumerate(kf.split(train_feats)):\n","                print(f'Epoch: {i + 1} Fold: {fold + 1}')\n","                X_train, y_train = train_feats.iloc[train_idx][train_cols], train_feats.iloc[train_idx][target_col]\n","                X_valid, y_valid = train_feats.iloc[valid_idx][train_cols], train_feats.iloc[valid_idx][target_col]\n","                params = {\n","                    \"objective\": \"reg:squarederror\",\n","                    \"eval_metric\": \"rmse\",\n","                    \"random_state\": 42,\n","                    \"verbosity\": 0,\n","                    **best_params\n","                }\n","                model = xgb.XGBRegressor(**params)\n","\n","                model.fit(\n","                    X_train, y_train,\n","                    eval_set=[(X_valid, y_valid)],\n","                    early_stopping_rounds=100,\n","                    verbose=False\n","                )\n","\n","                valid_predict = model.predict(X_valid)\n","                valid_preds[valid_idx] = valid_predict\n","                preds[valid_idx, 0] += valid_predict / EPOCHS\n","\n","                score = metrics.mean_squared_error(\n","                    y_valid, valid_predict, squared=False)\n","                model_dict[f'Epoch{i + 1}-Fold{fold + 1}'] = model\n","                model.save_model(\n","                    f'./baseline_xgb_models_optuna/xgb_model_epoch{i + 1}_fold{fold + 1}.json')\n","\n","            final_score = metrics.mean_squared_error(\n","                train_feats[target_col], valid_preds, squared=False)\n","            scores.append(final_score)\n","\n","        print(\"Avg Loss:\", np.mean(scores))\n","        return np.mean(scores)\n","\n","    study = optuna.create_study(direction=\"minimize\")\n","    study.optimize(objective, n_trials=200)\n","\n","    print(\"XGBoost Best trial:\")\n","    trial = study.best_trial\n","    print(f\"Value: {trial.value}\")\n","    print(\"Params: \")\n","    for key, value in trial.params.items():\n","        print(f\"{key}: {value}\")\n","    \n","    with open('xgb_best_params.json', 'w') as json_file:\n","        json.dump(trial.params, json_file, indent=4)\n","\n","    print(\"Save XGBoost best_params to json file\")"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":false,"trusted":true},"outputs":[],"source":["def train_cb_optuna(train_feats):\n","    os.makedirs('./baseline_cb_models_optuna', exist_ok=True)\n","\n","    def objective(trial):\n","        EPOCHS = 2\n","        SPLIT = 10\n","\n","        model_dict = {}\n","        scores = []\n","        preds = np.zeros((len(train_feats), 1))\n","\n","        best_params = {\n","            'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-3, 10.0),\n","            'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.1, 1.0),\n","            'subsample': trial.suggest_float('subsample', 0.1, 1.0),\n","            'learning_rate': trial.suggest_float('learning_rate', 0.0005, 1e-1, log=True),\n","            'depth': trial.suggest_int('depth', 1, 8),\n","            'iterations': trial.suggest_int('iterations', 1000, 15000),\n","            'min_child_samples': trial.suggest_int('min_child_samples', 1, 20),\n","            'thread_count': 4\n","        }\n","\n","        for i in range(EPOCHS):\n","            kf = model_selection.KFold(\n","                n_splits=SPLIT, random_state=42 + i * 10, shuffle=True)\n","            valid_preds = np.zeros(train_feats.shape[0])\n","\n","            for fold, (train_idx, valid_idx) in enumerate(kf.split(train_feats)):\n","                print(f'Epoch: {i + 1} Fold: {fold + 1}')\n","                X_train, y_train = train_feats.iloc[train_idx][train_cols], train_feats.iloc[train_idx][target_col]\n","                X_valid, y_valid = train_feats.iloc[valid_idx][train_cols], train_feats.iloc[valid_idx][target_col]\n","\n","                model = cb.CatBoostRegressor(\n","                    loss_function='RMSE',\n","                    random_seed=2023,\n","                    verbose=False,\n","                    **best_params\n","                )\n","\n","                model.fit(\n","                    X_train, y_train,\n","                    eval_set=[(X_valid, y_valid)],\n","                    early_stopping_rounds=100,\n","                    verbose=False\n","                )\n","\n","                valid_predict = model.predict(X_valid)\n","                valid_preds[valid_idx] = valid_predict\n","                preds[valid_idx, 0] += valid_predict / EPOCHS\n","\n","                score = metrics.mean_squared_error(\n","                    y_valid, valid_predict, squared=False)\n","                model_dict[f'Epoch{i + 1}-Fold{fold + 1}'] = model\n","                model.save_model(\n","                    f'./baseline_cb_models_optuna/cb_model_epoch{i + 1}_fold{fold + 1}.cbm')\n","\n","            final_score = metrics.mean_squared_error(\n","                train_feats[target_col], valid_preds, squared=False)\n","            scores.append(final_score)\n","\n","        print(\"Avg Loss:\", np.mean(scores))\n","        return np.mean(scores)\n","\n","    study = optuna.create_study(direction=\"minimize\")\n","    study.optimize(objective, n_trials=200)\n","\n","    print(\"CatBoost Best trial:\")\n","    trial = study.best_trial\n","    print(f\"Value: {trial.value}\")\n","    print(\"Params: \")\n","    for key, value in trial.params.items():\n","        print(f\"{key}: {value}\")\n","        \n","    with open('cb_best_params.json', 'w') as json_file:\n","        json.dump(trial.params, json_file, indent=4)\n","\n","    print(\"Save catboost best_params to json file\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["if CFG.is_train_lgbm_optuna:\n","    train_lgbm_optuna(train_feats=data)\n","\n","if CFG.is_train_xgb_optuna:\n","    train_xgb_optuna(train_feats=data)\n","\n","if CFG.is_train_cb_optuna:\n","    train_cb_optuna(train_feats=data)\n","\n","if CFG.is_train_lgbm_model:\n","    lgbm_preds = train_lgbm_model(train_feats=data, test_feats=test_feats)\n","\n","if CFG.is_train_xgb_model:\n","    xgb_preds = train_xgb_model(train_feats=data, test_feats=test_feats)\n","\n","if CFG.is_train_cb_model:\n","    cb_preds = train_cb_model(train_feats=data, test_feats=test_feats)"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":false,"trusted":true},"outputs":[],"source":["test_preds = np.zeros(test_feats.shape[0])\n","test_preds = lgbm_preds * 0.3 + xgb_preds * 0.3 + cb_preds * 0.4\n","\n","test_feats['score'] = test_preds\n","submission = test_feats[['id', 'score']]\n","submission.to_csv('submission.csv', index=False)\n","submission.head()"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":6678907,"sourceId":59291,"sourceType":"competition"}],"dockerImageVersionId":30626,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
