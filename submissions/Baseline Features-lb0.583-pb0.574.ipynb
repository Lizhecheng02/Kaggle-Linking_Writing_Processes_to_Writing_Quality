{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71d3bf9f",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-01-09T10:05:46.722950Z",
     "iopub.status.busy": "2024-01-09T10:05:46.722591Z",
     "iopub.status.idle": "2024-01-09T10:05:52.210830Z",
     "shell.execute_reply": "2024-01-09T10:05:52.209830Z"
    },
    "papermill": {
     "duration": 5.498801,
     "end_time": "2024-01-09T10:05:52.213298",
     "exception": false,
     "start_time": "2024-01-09T10:05:46.714497",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/opt/conda/lib/python3.10/site-packages/dask/dataframe/_pyarrow_compat.py:23: UserWarning: You are using pyarrow version 11.0.0 which is known to be insecure. See https://www.cve.org/CVERecord?id=CVE-2023-47248 for further details. Please upgrade to pyarrow>=14.0.1 or install pyarrow-hotfix to patch your current version.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from sklearn import metrics, model_selection\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "import optuna\n",
    "from scipy.stats import skew, kurtosis\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bba65a4e",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-01-09T10:05:52.229575Z",
     "iopub.status.busy": "2024-01-09T10:05:52.228750Z",
     "iopub.status.idle": "2024-01-09T10:05:52.233613Z",
     "shell.execute_reply": "2024-01-09T10:05:52.232741Z"
    },
    "papermill": {
     "duration": 0.015745,
     "end_time": "2024-01-09T10:05:52.235535",
     "exception": false,
     "start_time": "2024-01-09T10:05:52.219790",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    is_train_lgbm_model = True\n",
    "    is_train_lgbm_optuna = False\n",
    "    is_train_xgb_model = True\n",
    "    is_train_xgb_optuna = False\n",
    "    is_train_cb_model = True\n",
    "    is_train_cb_optuna = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e5e119b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T10:05:52.248729Z",
     "iopub.status.busy": "2024-01-09T10:05:52.248427Z",
     "iopub.status.idle": "2024-01-09T10:05:52.254044Z",
     "shell.execute_reply": "2024-01-09T10:05:52.253237Z"
    },
    "papermill": {
     "duration": 0.014402,
     "end_time": "2024-01-09T10:05:52.255915",
     "exception": false,
     "start_time": "2024-01-09T10:05:52.241513",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_cols = ['down_time', 'up_time', 'action_time',\n",
    "            'cursor_position', 'word_count']\n",
    "activities = ['Input', 'Remove/Cut', 'Nonproduction', 'Replace', 'Paste']\n",
    "events = ['q', 'Space', 'Backspace', 'Shift', 'ArrowRight', 'Leftclick', 'ArrowLeft',\n",
    "          '.', ',', 'ArrowDown', 'ArrowUp', 'Enter', 'CapsLock', \"'\", 'Delete', 'Unidentified']\n",
    "text_changes = ['q', ' ', '.', ',', '\\n', \"'\",\n",
    "                '\"', '-', '?', ';', '=', '/', '\\\\', ':']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f4ef037",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T10:05:52.269413Z",
     "iopub.status.busy": "2024-01-09T10:05:52.269107Z",
     "iopub.status.idle": "2024-01-09T10:05:52.300344Z",
     "shell.execute_reply": "2024-01-09T10:05:52.299405Z"
    },
    "papermill": {
     "duration": 0.040963,
     "end_time": "2024-01-09T10:05:52.302666",
     "exception": false,
     "start_time": "2024-01-09T10:05:52.261703",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_by_values(df, colname, values):\n",
    "    fts = df.select(pl.col('id').unique(maintain_order=True))\n",
    "    for i, value in enumerate(values):\n",
    "        tmp_df = df.group_by('id').agg(pl.col(colname).is_in(\n",
    "            [value]).sum().alias(f'{colname}_{i}_cnt'))\n",
    "        fts = fts.join(tmp_df, on='id', how='left')\n",
    "    return fts\n",
    "\n",
    "\n",
    "def dev_feats(df):\n",
    "    print(\"< Count by values features >\")\n",
    "\n",
    "    feats = count_by_values(df, 'activity', activities)\n",
    "    feats = feats.join(count_by_values(df, 'text_change',\n",
    "                       text_changes), on='id', how='left')\n",
    "    feats = feats.join(count_by_values(\n",
    "        df, 'down_event', events), on='id', how='left')\n",
    "    feats = feats.join(count_by_values(\n",
    "        df, 'up_event', events), on='id', how='left')\n",
    "\n",
    "    print(\"< Input words stats features >\")\n",
    "\n",
    "    temp = df.filter((~pl.col('text_change').str.contains('=>'))\n",
    "                     & (pl.col('text_change') != 'NoChange'))\n",
    "    temp = temp.group_by('id').agg(\n",
    "        pl.col('text_change').str.concat('').str.extract_all(r'q+'))\n",
    "    temp = temp.with_columns(\n",
    "        input_word_count=pl.col('text_change').list.lengths(),\n",
    "        input_word_length_mean=pl.col('text_change').apply(\n",
    "            lambda x: np.mean([len(i) for i in x] if len(x) > 0 else 0)\n",
    "        ),\n",
    "        input_word_length_max=pl.col('text_change').apply(\n",
    "            lambda x: np.max([len(i) for i in x] if len(x) > 0 else 0)\n",
    "        ),\n",
    "        input_word_length_std=pl.col('text_change').apply(\n",
    "            lambda x: np.std([len(i) for i in x] if len(x) > 0 else 0)\n",
    "        ),\n",
    "        input_word_length_median=pl.col('text_change').apply(\n",
    "            lambda x: np.median([len(i) for i in x] if len(x) > 0 else 0)\n",
    "        ),\n",
    "        input_word_length_skew=pl.col('text_change').apply(\n",
    "            lambda x: skew([len(i) for i in x] if len(x) > 0 else 0)\n",
    "        )\n",
    "    )\n",
    "    temp = temp.drop('text_change')\n",
    "    feats = feats.join(temp, on='id', how='left')\n",
    "\n",
    "    print(\"< Numerical columns features >\")\n",
    "\n",
    "    temp = df.group_by(\"id\").agg(\n",
    "        pl.sum('action_time').suffix('_sum'),\n",
    "        pl.mean(num_cols).suffix('_mean'),\n",
    "        pl.std(num_cols).suffix('_std'),\n",
    "        pl.median(num_cols).suffix('_median'),\n",
    "        pl.min(num_cols).suffix('_min'),\n",
    "        pl.max(num_cols).suffix('_max'),\n",
    "        pl.quantile(num_cols, 0.5).suffix('_quantile'))\n",
    "    feats = feats.join(temp, on='id', how='left')\n",
    "\n",
    "    print(\"< Categorical columns features >\")\n",
    "\n",
    "    temp = df.group_by(\"id\").agg(\n",
    "        pl.n_unique(['activity', 'down_event', 'up_event', 'text_change'])\n",
    "    )\n",
    "    feats = feats.join(temp, on='id', how='left')\n",
    "\n",
    "    print(\"< Idle time features >\")\n",
    "\n",
    "    temp = df.with_columns(pl.col('up_time').shift().over(\n",
    "        'id').alias('up_time_lagged'))\n",
    "    temp = temp.with_columns((abs(pl.col(\n",
    "        'down_time') - pl.col('up_time_lagged')) / 1000).fill_null(0).alias('time_diff'))\n",
    "    temp = temp.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n",
    "    temp = temp.group_by(\"id\").agg(\n",
    "        inter_key_largest_lantency=pl.max('time_diff'),\n",
    "        inter_key_median_lantency=pl.median('time_diff'),\n",
    "        mean_pause_time=pl.mean('time_diff'),\n",
    "        std_pause_time=pl.std('time_diff'),\n",
    "        total_pause_time=pl.sum('time_diff'),\n",
    "        pauses_half_sec=pl.col('time_diff').filter(\n",
    "            (pl.col('time_diff') > 0.5) & (pl.col('time_diff') < 1)\n",
    "        ).count(),\n",
    "        pauses_1_sec=pl.col('time_diff').filter(\n",
    "            (pl.col('time_diff') > 1) & (pl.col('time_diff') < 1.5)\n",
    "        ).count(),\n",
    "        pauses_1_half_sec=pl.col('time_diff').filter(\n",
    "            (pl.col('time_diff') > 1.5) & (pl.col('time_diff') < 2)\n",
    "        ).count(),\n",
    "        pauses_2_sec=pl.col('time_diff').filter(\n",
    "            (pl.col('time_diff') > 2) & (pl.col('time_diff') < 3)\n",
    "        ).count(),\n",
    "        pauses_3_sec=pl.col('time_diff').filter(\n",
    "            pl.col('time_diff') > 3).count()\n",
    "    )\n",
    "    feats = feats.join(temp, on='id', how='left')\n",
    "\n",
    "    print(\"< P-bursts features >\")\n",
    "\n",
    "    temp = df.with_columns(pl.col('up_time').shift().over(\n",
    "        'id').alias('up_time_lagged'))\n",
    "    temp = temp.with_columns((abs(pl.col(\n",
    "        'down_time') - pl.col('up_time_lagged')) / 1000).fill_null(0).alias('time_diff'))\n",
    "    temp = temp.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n",
    "    temp = temp.with_columns(pl.col('time_diff') < 2)\n",
    "    temp = temp.with_columns(pl.when(pl.col(\"time_diff\") & pl.col(\"time_diff\").is_last(\n",
    "    )).then(pl.count()).over(pl.col(\"time_diff\").rle_id()).alias('P-bursts'))\n",
    "    temp = temp.drop_nulls()\n",
    "    temp = temp.group_by(\"id\").agg(\n",
    "        pl.mean('P-bursts').suffix('_mean'),\n",
    "        pl.std('P-bursts').suffix('_std'),\n",
    "        pl.count('P-bursts').suffix('_count'),\n",
    "        pl.median('P-bursts').suffix('_median'),\n",
    "        pl.max('P-bursts').suffix('_max'),\n",
    "        pl.first('P-bursts').suffix('_first'),\n",
    "        pl.last('P-bursts').suffix('_last')\n",
    "    )\n",
    "    feats = feats.join(temp, on='id', how='left')\n",
    "\n",
    "    print(\"< R-bursts features >\")\n",
    "\n",
    "    temp = df.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n",
    "    temp = temp.with_columns(pl.col('activity').is_in(['Remove/Cut']))\n",
    "    temp = temp.with_columns(pl.when(pl.col(\"activity\") & pl.col(\"activity\").is_last(\n",
    "    )).then(pl.count()).over(pl.col(\"activity\").rle_id()).alias('R-bursts'))\n",
    "    temp = temp.drop_nulls()\n",
    "    temp = temp.group_by(\"id\").agg(\n",
    "        pl.mean('R-bursts').suffix('_mean'),\n",
    "        pl.std('R-bursts').suffix('_std'),\n",
    "        pl.median('R-bursts').suffix('_median'),\n",
    "        pl.max('R-bursts').suffix('_max'),\n",
    "        pl.first('R-bursts').suffix('_first'),\n",
    "        pl.last('R-bursts').suffix('_last')\n",
    "    )\n",
    "    feats = feats.join(temp, on='id', how='left')\n",
    "\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09d6a31b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T10:05:52.316343Z",
     "iopub.status.busy": "2024-01-09T10:05:52.316023Z",
     "iopub.status.idle": "2024-01-09T10:05:52.346840Z",
     "shell.execute_reply": "2024-01-09T10:05:52.346063Z"
    },
    "papermill": {
     "duration": 0.040221,
     "end_time": "2024-01-09T10:05:52.348904",
     "exception": false,
     "start_time": "2024-01-09T10:05:52.308683",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def q1(x):\n",
    "    return x.quantile(0.25)\n",
    "\n",
    "\n",
    "def q3(x):\n",
    "    return x.quantile(0.75)\n",
    "\n",
    "\n",
    "AGGREGATIONS = ['count', 'mean', 'min', 'max', 'first', 'last', q1, 'median', q3, 'sum']\n",
    "\n",
    "\n",
    "def reconstruct_essay(currTextInput):\n",
    "    essayText = \"\"\n",
    "    for Input in currTextInput.values:\n",
    "        if Input[0] == 'Replace':\n",
    "            replaceTxt = Input[2].split(' => ')\n",
    "            essayText = essayText[:Input[1] - len(replaceTxt[1])] + replaceTxt[1] + \\\n",
    "                essayText[Input[1] - len(replaceTxt[1]) + len(replaceTxt[0]):]\n",
    "            continue\n",
    "        if Input[0] == 'Paste':\n",
    "            essayText = essayText[:Input[1] - len(Input[2])] + \\\n",
    "                Input[2] + essayText[Input[1] - len(Input[2]):]\n",
    "            continue\n",
    "        if Input[0] == 'Remove/Cut':\n",
    "            essayText = essayText[:Input[1]] + \\\n",
    "                essayText[Input[1] + len(Input[2]):]\n",
    "            continue\n",
    "        if \"M\" in Input[0]:\n",
    "            croppedTxt = Input[0][10:]\n",
    "            splitTxt = croppedTxt.split(' To ')\n",
    "            valueArr = [item.split(', ') for item in splitTxt]\n",
    "            moveData = (int(valueArr[0][0][1:]), int(\n",
    "                valueArr[0][1][:-1]), int(valueArr[1][0][1:]), int(valueArr[1][1][:-1]))\n",
    "            if moveData[0] != moveData[2]:\n",
    "                if moveData[0] < moveData[2]:\n",
    "                    essayText = essayText[:moveData[0]] + \\\n",
    "                        essayText[moveData[1]:moveData[3]] + \\\n",
    "                        essayText[moveData[0]:moveData[1]] + \\\n",
    "                        essayText[moveData[3]:]\n",
    "                else:\n",
    "                    essayText = essayText[:moveData[2]] + \\\n",
    "                        essayText[moveData[0]:moveData[1]] + \\\n",
    "                        essayText[moveData[2]:moveData[0]] + \\\n",
    "                        essayText[moveData[1]:]\n",
    "            continue\n",
    "        essayText = essayText[:Input[1] - len(Input[2])] + \\\n",
    "            Input[2] + essayText[Input[1] - len(Input[2]):]\n",
    "    return essayText\n",
    "\n",
    "\n",
    "def get_essay_df(df):\n",
    "    df = df[df.activity != 'Nonproduction']\n",
    "    temp = df.groupby('id').apply(lambda x: reconstruct_essay(\n",
    "        x[['activity', 'cursor_position', 'text_change']]))\n",
    "    essay_df = pd.DataFrame({'id': df['id'].unique().tolist()})\n",
    "    essay_df = essay_df.merge(temp.rename('essay'), on='id')\n",
    "    return essay_df\n",
    "\n",
    "\n",
    "def word_feats(df):\n",
    "    essay_df = df\n",
    "    df['word'] = df['essay'].apply(lambda x: re.split(' |\\\\n|\\\\.|\\\\?|\\\\!', x))\n",
    "    df = df.explode('word')\n",
    "    df['word_len'] = df['word'].apply(lambda x: len(x))\n",
    "    df = df[df['word_len'] != 0]\n",
    "\n",
    "    word_agg_df = df[['id', 'word_len']].groupby(['id']).agg(AGGREGATIONS)\n",
    "    word_agg_df.columns = ['_'.join(x) for x in word_agg_df.columns]\n",
    "    word_agg_df['id'] = word_agg_df.index\n",
    "    word_agg_df = word_agg_df.reset_index(drop=True)\n",
    "    return word_agg_df\n",
    "\n",
    "\n",
    "def sent_feats(df):\n",
    "    df['sent'] = df['essay'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!', x))\n",
    "    df = df.explode('sent')\n",
    "    df['sent'] = df['sent'].apply(lambda x: x.replace('\\n', '').strip())\n",
    "    # Number of characters in sentences\n",
    "    df['sent_len'] = df['sent'].apply(lambda x: len(x))\n",
    "    # Number of words in sentences\n",
    "    df['sent_word_count'] = df['sent'].apply(lambda x: len(x.split(' ')))\n",
    "    df = df[df.sent_len != 0].reset_index(drop=True)\n",
    "\n",
    "    sent_agg_df = pd.concat([df[['id', 'sent_len']].groupby(['id']).agg(AGGREGATIONS),\n",
    "                             df[['id', 'sent_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1)\n",
    "    sent_agg_df.columns = ['_'.join(x) for x in sent_agg_df.columns]\n",
    "    sent_agg_df['id'] = sent_agg_df.index\n",
    "    sent_agg_df = sent_agg_df.reset_index(drop=True)\n",
    "    sent_agg_df.drop(columns=[\"sent_word_count_count\"], inplace=True)\n",
    "    sent_agg_df = sent_agg_df.rename(columns={\"sent_len_count\": \"sent_count\"})\n",
    "    return sent_agg_df\n",
    "\n",
    "\n",
    "def parag_feats(df):\n",
    "    df['paragraph'] = df['essay'].apply(lambda x: x.split('\\n'))\n",
    "    df = df.explode('paragraph')\n",
    "    # Number of characters in paragraphs\n",
    "    df['paragraph_len'] = df['paragraph'].apply(lambda x: len(x))\n",
    "    # Number of words in paragraphs\n",
    "    df['paragraph_word_count'] = df['paragraph'].apply(\n",
    "        lambda x: len(x.split(' ')))\n",
    "    df = df[df.paragraph_len != 0].reset_index(drop=True)\n",
    "\n",
    "    paragraph_agg_df = pd.concat([df[['id', 'paragraph_len']].groupby(['id']).agg(AGGREGATIONS),\n",
    "                                  df[['id', 'paragraph_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1)\n",
    "    paragraph_agg_df.columns = ['_'.join(x) for x in paragraph_agg_df.columns]\n",
    "    paragraph_agg_df['id'] = paragraph_agg_df.index\n",
    "    paragraph_agg_df = paragraph_agg_df.reset_index(drop=True)\n",
    "    paragraph_agg_df.drop(columns=[\"paragraph_word_count_count\"], inplace=True)\n",
    "    paragraph_agg_df = paragraph_agg_df.rename(\n",
    "        columns={\"paragraph_len_count\": \"paragraph_count\"})\n",
    "    return paragraph_agg_df\n",
    "\n",
    "\n",
    "def product_to_keys(logs, essays):\n",
    "    essays['product_len'] = essays.essay.str.len()\n",
    "    tmp_df = logs[logs.activity.isin(['Input', 'Remove/Cut'])].groupby(['id']).agg(\n",
    "        {'activity': 'count'}).reset_index().rename(columns={'activity': 'keys_pressed'})\n",
    "    essays = essays.merge(tmp_df, on='id', how='left')\n",
    "    essays['product_to_keys'] = essays['product_len'] / essays['keys_pressed']\n",
    "    return essays[['id', 'product_to_keys']]\n",
    "\n",
    "\n",
    "def get_keys_pressed_per_second(logs):\n",
    "    temp_df = logs[logs['activity'].isin(['Input', 'Remove/Cut'])].groupby(\n",
    "        ['id']).agg(keys_pressed=('event_id', 'count')).reset_index()\n",
    "    temp_df_2 = logs.groupby(['id']).agg(min_down_time=(\n",
    "        'down_time', 'min'), max_up_time=('up_time', 'max')).reset_index()\n",
    "    temp_df = temp_df.merge(temp_df_2, on='id', how='left')\n",
    "    temp_df['keys_per_second'] = temp_df['keys_pressed'] / \\\n",
    "        ((temp_df['max_up_time'] - temp_df['min_down_time']) / 1000)\n",
    "    return temp_df[['id', 'keys_per_second']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "753a29d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T10:05:52.362343Z",
     "iopub.status.busy": "2024-01-09T10:05:52.361893Z",
     "iopub.status.idle": "2024-01-09T10:07:11.393229Z",
     "shell.execute_reply": "2024-01-09T10:07:11.392208Z"
    },
    "papermill": {
     "duration": 79.046249,
     "end_time": "2024-01-09T10:07:11.401222",
     "exception": false,
     "start_time": "2024-01-09T10:05:52.354973",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< Count by values features >\n",
      "< Input words stats features >\n",
      "< Numerical columns features >\n",
      "< Categorical columns features >\n",
      "< Idle time features >\n",
      "< P-bursts features >\n",
      "< R-bursts features >\n",
      "< Essay Reconstruction >\n",
      "< Mapping >\n",
      "Number of features: 165\n",
      "< Testing Data >\n",
      "< Count by values features >\n",
      "< Input words stats features >\n",
      "< Numerical columns features >\n",
      "< Categorical columns features >\n",
      "< Idle time features >\n",
      "< P-bursts features >\n",
      "< R-bursts features >\n"
     ]
    }
   ],
   "source": [
    "data_path = '/kaggle/input/linking-writing-processes-to-writing-quality/'\n",
    "train_logs = pl.scan_csv(data_path + 'train_logs.csv')\n",
    "train_feats = dev_feats(train_logs)\n",
    "train_feats = train_feats.collect().to_pandas()\n",
    "\n",
    "print('< Essay Reconstruction >')\n",
    "train_logs = train_logs.collect().to_pandas()\n",
    "train_essays = get_essay_df(train_logs)\n",
    "train_feats = train_feats.merge(word_feats(train_essays), on='id', how='left')\n",
    "train_feats = train_feats.merge(sent_feats(train_essays), on='id', how='left')\n",
    "train_feats = train_feats.merge(parag_feats(train_essays), on='id', how='left')\n",
    "train_feats = train_feats.merge(\n",
    "    get_keys_pressed_per_second(train_logs), on='id', how='left')\n",
    "train_feats = train_feats.merge(product_to_keys(\n",
    "    train_logs, train_essays), on='id', how='left')\n",
    "# train_feats = train_feats.fillna(0.0)\n",
    "\n",
    "print('< Mapping >')\n",
    "train_scores = pd.read_csv(data_path + 'train_scores.csv')\n",
    "data = train_feats.merge(train_scores, on='id', how='left')\n",
    "x = data.drop(['id', 'score'], axis=1)\n",
    "y = data['score'].values\n",
    "print(f'Number of features: {len(x.columns)}')\n",
    "\n",
    "\n",
    "print('< Testing Data >')\n",
    "test_logs = pl.scan_csv(data_path + 'test_logs.csv')\n",
    "test_feats = dev_feats(test_logs)\n",
    "test_feats = test_feats.collect().to_pandas()\n",
    "\n",
    "test_logs = test_logs.collect().to_pandas()\n",
    "test_essays = get_essay_df(test_logs)\n",
    "test_feats = test_feats.merge(word_feats(test_essays), on='id', how='left')\n",
    "test_feats = test_feats.merge(sent_feats(test_essays), on='id', how='left')\n",
    "test_feats = test_feats.merge(parag_feats(test_essays), on='id', how='left')\n",
    "test_feats = test_feats.merge(\n",
    "    get_keys_pressed_per_second(test_logs), on='id', how='left')\n",
    "test_feats = test_feats.merge(product_to_keys(\n",
    "    test_logs, test_essays), on='id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14dca8d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T10:07:11.416186Z",
     "iopub.status.busy": "2024-01-09T10:07:11.415728Z",
     "iopub.status.idle": "2024-01-09T10:07:11.421562Z",
     "shell.execute_reply": "2024-01-09T10:07:11.420612Z"
    },
    "papermill": {
     "duration": 0.016172,
     "end_time": "2024-01-09T10:07:11.423922",
     "exception": false,
     "start_time": "2024-01-09T10:07:11.407750",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< Learning and Evaluation >\n"
     ]
    }
   ],
   "source": [
    "target_col = ['score']\n",
    "drop_cols = ['id']\n",
    "train_cols = [\n",
    "    col for col in train_feats.columns if col not in target_col + drop_cols\n",
    "]\n",
    "\n",
    "print('< Learning and Evaluation >')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b13cd29e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T10:07:11.439424Z",
     "iopub.status.busy": "2024-01-09T10:07:11.438637Z",
     "iopub.status.idle": "2024-01-09T10:07:11.454072Z",
     "shell.execute_reply": "2024-01-09T10:07:11.453152Z"
    },
    "papermill": {
     "duration": 0.025817,
     "end_time": "2024-01-09T10:07:11.456526",
     "exception": false,
     "start_time": "2024-01-09T10:07:11.430709",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_lgbm_model(train_feats, test_feats):\n",
    "    print(\"Training LightGBM Model ..........\")\n",
    "    TEST_PREDS = np.zeros((len(test_feats), 1))\n",
    "\n",
    "    os.makedirs('./baseline_lgb_models', exist_ok=True)\n",
    "\n",
    "    EPOCHS = 3\n",
    "    SPLIT = 10\n",
    "\n",
    "    test_prediction_list = []\n",
    "    model_dict = {}\n",
    "    scores = []\n",
    "    preds = np.zeros((len(train_feats), 1))\n",
    "\n",
    "    best_params = {\n",
    "        \"reg_alpha\": 0.20942197105796304,\n",
    "        \"reg_lambda\": 1.1041148045159934,\n",
    "        \"colsample_bytree\": 0.4755550885867577,\n",
    "        \"subsample\": 0.7747751125780099,\n",
    "        \"learning_rate\": 0.02281036909201476,\n",
    "        \"num_leaves\": 13,\n",
    "        \"max_depth\": 30,\n",
    "        \"min_child_samples\": 19,\n",
    "        \"n_estimators\": 12289\n",
    "    }\n",
    "\n",
    "    for i in range(EPOCHS):\n",
    "        kf = model_selection.KFold(\n",
    "            n_splits=SPLIT, random_state=42 + i * 10, shuffle=True)\n",
    "        valid_preds = np.zeros(train_feats.shape[0])\n",
    "        X_test = test_feats[train_cols]\n",
    "\n",
    "        for fold, (train_idx, valid_idx) in enumerate(kf.split(train_feats)):\n",
    "            print(f'Epoch: {i + 1} Fold: {fold + 1}')\n",
    "            X_train, y_train = train_feats.iloc[train_idx][train_cols], train_feats.iloc[train_idx][target_col]\n",
    "            X_valid, y_valid = train_feats.iloc[valid_idx][train_cols], train_feats.iloc[valid_idx][target_col]\n",
    "            params = {\n",
    "                \"objective\": \"regression\",\n",
    "                \"metric\": \"rmse\",\n",
    "                \"random_state\": 42,\n",
    "                \"verbosity\": 1,\n",
    "                **best_params\n",
    "            }\n",
    "            model = lgb.LGBMRegressor(**params)\n",
    "            early_stopping_callback = lgb.early_stopping(\n",
    "                100, first_metric_only=True, verbose=True\n",
    "            )\n",
    "\n",
    "            model.fit(\n",
    "                X_train, y_train,\n",
    "                eval_set=[(X_valid, y_valid)],\n",
    "                callbacks=[early_stopping_callback]\n",
    "            )\n",
    "\n",
    "            valid_predict = model.predict(X_valid)\n",
    "            valid_preds[valid_idx] = valid_predict\n",
    "            preds[valid_idx, 0] += valid_predict / EPOCHS\n",
    "\n",
    "            test_predict = model.predict(X_test)\n",
    "            TEST_PREDS[:, 0] += test_predict / EPOCHS / SPLIT\n",
    "            test_prediction_list.append(test_predict)\n",
    "\n",
    "            score = metrics.mean_squared_error(\n",
    "                y_valid, valid_predict, squared=False)\n",
    "            model_dict[f'Epoch{i + 1}-Fold{fold + 1}'] = model\n",
    "            model.booster_.save_model(\n",
    "                f'./baseline_lgb_models/lgbm_model_epoch{i + 1}_fold{fold + 1}.txt')\n",
    "\n",
    "        final_score = metrics.mean_squared_error(\n",
    "            train_feats[target_col], valid_preds, squared=False)\n",
    "        scores.append(final_score)\n",
    "\n",
    "    print(\"Avg Loss:\", np.mean(scores))\n",
    "\n",
    "    print('metric LGBM = {:.5f}'.format(metrics.mean_squared_error(\n",
    "        train_feats[target_col], preds[:, 0], squared=False)))\n",
    "\n",
    "    return TEST_PREDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72410bac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T10:07:11.472426Z",
     "iopub.status.busy": "2024-01-09T10:07:11.471551Z",
     "iopub.status.idle": "2024-01-09T10:07:11.486367Z",
     "shell.execute_reply": "2024-01-09T10:07:11.485306Z"
    },
    "papermill": {
     "duration": 0.025056,
     "end_time": "2024-01-09T10:07:11.488374",
     "exception": false,
     "start_time": "2024-01-09T10:07:11.463318",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_xgb_model(train_feats, test_feats):\n",
    "    print(\"Training XGBoost Model ..........\")\n",
    "    TEST_PREDS = np.zeros((len(test_feats), 1))\n",
    "\n",
    "    os.makedirs('./baseline_xgb_models', exist_ok=True)\n",
    "\n",
    "    EPOCHS = 3\n",
    "    SPLIT = 10\n",
    "\n",
    "    test_prediction_list = []\n",
    "    model_dict = {}\n",
    "    scores = []\n",
    "    preds = np.zeros((len(train_feats), 1))\n",
    "\n",
    "    best_params = {\n",
    "        \"reg_alpha\": 0.6388999390828651,\n",
    "        \"reg_lambda\": 3.6779665204618652,\n",
    "        \"colsample_bytree\": 0.782542055226585,\n",
    "        \"subsample\": 0.43793835238756645,\n",
    "        \"learning_rate\": 0.002147349844538151,\n",
    "        \"max_depth\": 28,\n",
    "        \"min_child_weight\": 1.706377795500605,\n",
    "        \"gamma\": 1.763643817812136,\n",
    "        \"max_delta_step\": 5,\n",
    "        \"n_estimators\": 8497\n",
    "    }\n",
    "\n",
    "    for i in range(EPOCHS):\n",
    "        kf = model_selection.KFold(\n",
    "            n_splits=SPLIT, random_state=42 + i * 10, shuffle=True)\n",
    "        valid_preds = np.zeros(train_feats.shape[0])\n",
    "        X_test = test_feats[train_cols]\n",
    "\n",
    "        for fold, (train_idx, valid_idx) in enumerate(kf.split(train_feats)):\n",
    "            print(f'Epoch: {i + 1} Fold: {fold + 1}')\n",
    "            X_train, y_train = train_feats.iloc[train_idx][train_cols], train_feats.iloc[train_idx][target_col]\n",
    "            X_valid, y_valid = train_feats.iloc[valid_idx][train_cols], train_feats.iloc[valid_idx][target_col]\n",
    "            params = {\n",
    "                \"objective\": \"reg:squarederror\",\n",
    "                \"eval_metric\": \"rmse\",\n",
    "                \"random_state\": 42,\n",
    "                \"verbosity\": 0,\n",
    "                **best_params\n",
    "            }\n",
    "            model = xgb.XGBRegressor(**params)\n",
    "\n",
    "            model.fit(\n",
    "                X_train, y_train,\n",
    "                eval_set=[(X_valid, y_valid)],\n",
    "                early_stopping_rounds=100,\n",
    "                verbose=False\n",
    "            )\n",
    "\n",
    "            valid_predict = model.predict(X_valid)\n",
    "            valid_preds[valid_idx] = valid_predict\n",
    "            preds[valid_idx, 0] += valid_predict / EPOCHS\n",
    "\n",
    "            test_predict = model.predict(X_test)\n",
    "            TEST_PREDS[:, 0] += test_predict / EPOCHS / SPLIT\n",
    "            test_prediction_list.append(test_predict)\n",
    "\n",
    "            score = metrics.mean_squared_error(\n",
    "                y_valid, valid_predict, squared=False)\n",
    "            model_dict[f'Epoch{i + 1}-Fold{fold + 1}'] = model\n",
    "            model.save_model(\n",
    "                f'./baseline_xgb_models/xgb_model_epoch{i + 1}_fold{fold + 1}.json')\n",
    "            # model.load_model(f'./baseline_xgb_models/xgb_model_epoch{i + 1}_fold{fold + 1}.json')\n",
    "\n",
    "        final_score = metrics.mean_squared_error(\n",
    "            train_feats[target_col], valid_preds, squared=False)\n",
    "        scores.append(final_score)\n",
    "\n",
    "    print(\"Avg Loss:\", np.mean(scores))\n",
    "\n",
    "    print('metric XGB = {:.5f}'.format(metrics.mean_squared_error(\n",
    "        train_feats[target_col], preds[:, 0], squared=False)))\n",
    "\n",
    "    return TEST_PREDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a36c1309",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T10:07:11.504907Z",
     "iopub.status.busy": "2024-01-09T10:07:11.504498Z",
     "iopub.status.idle": "2024-01-09T10:07:11.519313Z",
     "shell.execute_reply": "2024-01-09T10:07:11.518354Z"
    },
    "papermill": {
     "duration": 0.024781,
     "end_time": "2024-01-09T10:07:11.521388",
     "exception": false,
     "start_time": "2024-01-09T10:07:11.496607",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_cb_model(train_feats, test_feats):\n",
    "    print(\"Training CatBoost Model ..........\")\n",
    "    TEST_PREDS = np.zeros((len(test_feats), 1))\n",
    "\n",
    "    os.makedirs('./baseline_cb_models', exist_ok=True)\n",
    "\n",
    "    EPOCHS = 3\n",
    "    SPLIT = 10\n",
    "\n",
    "    test_prediction_list = []\n",
    "    model_dict = {}\n",
    "    scores = []\n",
    "    preds = np.zeros((len(train_feats), 1))\n",
    "\n",
    "    best_params = {\n",
    "        \"l2_leaf_reg\": 2.869221700701507,\n",
    "        \"colsample_bylevel\": 0.8275269672701923,\n",
    "        \"subsample\": 0.23518262280165447,\n",
    "        \"learning_rate\": 0.0274498466487788,\n",
    "        \"depth\": 6,\n",
    "        \"iterations\": 9812,\n",
    "        \"min_child_samples\": 2\n",
    "    }\n",
    "\n",
    "    for i in range(EPOCHS):\n",
    "        kf = model_selection.KFold(\n",
    "            n_splits=SPLIT, random_state=42 + i * 10, shuffle=True)\n",
    "        valid_preds = np.zeros(train_feats.shape[0])\n",
    "        X_test = test_feats[train_cols]\n",
    "\n",
    "        for fold, (train_idx, valid_idx) in enumerate(kf.split(train_feats)):\n",
    "            print(f'Epoch: {i + 1} Fold: {fold + 1}')\n",
    "            X_train, y_train = train_feats.iloc[train_idx][train_cols], train_feats.iloc[train_idx][target_col]\n",
    "            X_valid, y_valid = train_feats.iloc[valid_idx][train_cols], train_feats.iloc[valid_idx][target_col]\n",
    "\n",
    "            model = cb.CatBoostRegressor(\n",
    "                loss_function='RMSE',\n",
    "                random_seed=2023,\n",
    "                verbose=False,\n",
    "                **best_params\n",
    "            )\n",
    "\n",
    "            model.fit(\n",
    "                X_train, y_train,\n",
    "                eval_set=[(X_valid, y_valid)],\n",
    "                early_stopping_rounds=100,\n",
    "                verbose=False\n",
    "            )\n",
    "\n",
    "            valid_predict = model.predict(X_valid)\n",
    "            valid_preds[valid_idx] = valid_predict\n",
    "            preds[valid_idx, 0] += valid_predict / EPOCHS\n",
    "\n",
    "            test_predict = model.predict(X_test)\n",
    "            TEST_PREDS[:, 0] += test_predict / EPOCHS / SPLIT\n",
    "            test_prediction_list.append(test_predict)\n",
    "\n",
    "            score = metrics.mean_squared_error(\n",
    "                y_valid, valid_predict, squared=False)\n",
    "            model_dict[f'Epoch{i + 1}-Fold{fold + 1}'] = model\n",
    "            model.save_model(\n",
    "                f'./baseline_cb_models/cb_model_epoch{i + 1}_fold{fold + 1}.cbm')\n",
    "            # model.load_model(f'./baseline_cb_models/cb_model_epoch{i + 1}_fold{fold + 1}.cbm')\n",
    "\n",
    "        final_score = metrics.mean_squared_error(\n",
    "            train_feats[target_col], valid_preds, squared=False)\n",
    "        scores.append(final_score)\n",
    "\n",
    "    print(\"Avg Loss:\", np.mean(scores))\n",
    "\n",
    "    print('metric CB = {:.5f}'.format(metrics.mean_squared_error(\n",
    "        train_feats[target_col], preds[:, 0], squared=False)))\n",
    "\n",
    "    return TEST_PREDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "488b88de",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-01-09T10:07:11.536595Z",
     "iopub.status.busy": "2024-01-09T10:07:11.536239Z",
     "iopub.status.idle": "2024-01-09T10:07:11.552458Z",
     "shell.execute_reply": "2024-01-09T10:07:11.551500Z"
    },
    "papermill": {
     "duration": 0.026613,
     "end_time": "2024-01-09T10:07:11.554691",
     "exception": false,
     "start_time": "2024-01-09T10:07:11.528078",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_lgbm_optuna(train_feats):\n",
    "    os.makedirs('./baseline_lgb_models_optuna', exist_ok=True)\n",
    "\n",
    "    def objective(trial):\n",
    "        EPOCHS = 1\n",
    "        SPLIT = 10\n",
    "\n",
    "        model_dict = {}\n",
    "        scores = []\n",
    "        preds = np.zeros((len(train_feats), 1))\n",
    "\n",
    "        best_params = {\n",
    "            'reg_alpha': trial.suggest_float(\"reg_alpha\", 0.0, 1.0),\n",
    "            'reg_lambda': trial.suggest_float(\"reg_lambda\", 0.0, 5.0),\n",
    "            'colsample_bytree': trial.suggest_float(\"colsample_bytree\", 0.4, 1.0),\n",
    "            'subsample': trial.suggest_float(\"subsample\", 0.4, 1.0),\n",
    "            'learning_rate': trial.suggest_float(\"learning_rate\", 0.0001, 1.0),\n",
    "            'num_leaves': trial.suggest_int(\"num_leaves\", 5, 50),\n",
    "            'max_depth': trial.suggest_int(\"max_depth\", 5, 30),\n",
    "            'min_child_samples': trial.suggest_int(\"min_child_samples\", 2, 30),\n",
    "            'n_jobs': 4,\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 1000, 20000)\n",
    "        }\n",
    "\n",
    "        for i in range(EPOCHS):\n",
    "            kf = model_selection.KFold(\n",
    "                n_splits=SPLIT, random_state=42 + i * 10, shuffle=True)\n",
    "            valid_preds = np.zeros(train_feats.shape[0])\n",
    "\n",
    "            for fold, (train_idx, valid_idx) in enumerate(kf.split(train_feats)):\n",
    "                print(f'Epoch: {i + 1} Fold: {fold + 1}')\n",
    "                X_train, y_train = train_feats.iloc[train_idx][train_cols], train_feats.iloc[train_idx][target_col]\n",
    "                X_valid, y_valid = train_feats.iloc[valid_idx][train_cols], train_feats.iloc[valid_idx][target_col]\n",
    "                params = {\n",
    "                    \"objective\": \"regression\",\n",
    "                    \"metric\": \"rmse\",\n",
    "                    \"random_state\": 42,\n",
    "                    \"verbosity\": 1,\n",
    "                    **best_params\n",
    "                }\n",
    "                model = lgb.LGBMRegressor(**params)\n",
    "                early_stopping_callback = lgb.early_stopping(\n",
    "                    100, first_metric_only=True, verbose=True\n",
    "                )\n",
    "\n",
    "                model.fit(\n",
    "                    X_train, y_train,\n",
    "                    eval_set=[(X_valid, y_valid)],\n",
    "                    callbacks=[early_stopping_callback]\n",
    "                )\n",
    "\n",
    "                valid_predict = model.predict(X_valid)\n",
    "                valid_preds[valid_idx] = valid_predict\n",
    "                preds[valid_idx, 0] += valid_predict / EPOCHS\n",
    "\n",
    "                score = metrics.mean_squared_error(\n",
    "                    y_valid, valid_predict, squared=False)\n",
    "                model_dict[f'Epoch{i + 1}-Fold{fold + 1}'] = model\n",
    "                model.booster_.save_model(\n",
    "                    f'./baseline_lgb_models_optuna/lgbm_model_epoch{i + 1}_fold{fold + 1}.txt')\n",
    "\n",
    "            final_score = metrics.mean_squared_error(\n",
    "                train_feats[target_col], valid_preds, squared=False)\n",
    "            scores.append(final_score)\n",
    "\n",
    "        print(\"Avg Loss:\", np.mean(scores))\n",
    "        return np.mean(scores)\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=25)\n",
    "\n",
    "    print(\"LightGBM Best trial:\")\n",
    "    trial = study.best_trial\n",
    "    print(f\"Value: {trial.value}\")\n",
    "    print(\"Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    \n",
    "    with open('lgbm_best_params.json', 'w') as json_file:\n",
    "        json.dump(trial.params, json_file, indent=4)\n",
    "\n",
    "    print(\"Save LightGBM best_params to json file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1955a73d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T10:07:11.570033Z",
     "iopub.status.busy": "2024-01-09T10:07:11.569095Z",
     "iopub.status.idle": "2024-01-09T10:07:11.586386Z",
     "shell.execute_reply": "2024-01-09T10:07:11.585356Z"
    },
    "papermill": {
     "duration": 0.027456,
     "end_time": "2024-01-09T10:07:11.588772",
     "exception": false,
     "start_time": "2024-01-09T10:07:11.561316",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_xgb_optuna(train_feats):\n",
    "    os.makedirs('./baseline_xgb_models_optuna', exist_ok=True)\n",
    "\n",
    "    def objective(trial):\n",
    "        EPOCHS = 1\n",
    "        SPLIT = 10\n",
    "\n",
    "        model_dict = {}\n",
    "        scores = []\n",
    "        preds = np.zeros((len(train_feats), 1))\n",
    "\n",
    "        best_params = {\n",
    "            'reg_alpha': trial.suggest_float(\"reg_alpha\", 0.0, 1.0),\n",
    "            'reg_lambda': trial.suggest_float(\"reg_lambda\", 0.0, 5.0),\n",
    "            'colsample_bytree': trial.suggest_float(\"colsample_bytree\", 0.4, 1.0),\n",
    "            'subsample': trial.suggest_float(\"subsample\", 0.4, 1.0),\n",
    "            'learning_rate': trial.suggest_float(\"learning_rate\", 0.0001, 1.0),\n",
    "            'max_depth': trial.suggest_int(\"max_depth\", 5, 30),\n",
    "            'min_child_weight': trial.suggest_float(\"min_child_weight\", 1.0, 5.0),\n",
    "            'gamma': trial.suggest_float(\"gamma\", 0.0, 10.0),\n",
    "            'max_delta_step': trial.suggest_int(\"max_delta_step\", 1, 5),\n",
    "            'n_jobs': 4,\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 1000, 20000)\n",
    "        }\n",
    "\n",
    "        for i in range(EPOCHS):\n",
    "            kf = model_selection.KFold(\n",
    "                n_splits=SPLIT, random_state=42 + i * 10, shuffle=True)\n",
    "            valid_preds = np.zeros(train_feats.shape[0])\n",
    "\n",
    "            for fold, (train_idx, valid_idx) in enumerate(kf.split(train_feats)):\n",
    "                print(f'Epoch: {i + 1} Fold: {fold + 1}')\n",
    "                X_train, y_train = train_feats.iloc[train_idx][train_cols], train_feats.iloc[train_idx][target_col]\n",
    "                X_valid, y_valid = train_feats.iloc[valid_idx][train_cols], train_feats.iloc[valid_idx][target_col]\n",
    "                params = {\n",
    "                    \"objective\": \"reg:squarederror\",\n",
    "                    \"eval_metric\": \"rmse\",\n",
    "                    \"random_state\": 42,\n",
    "                    \"verbosity\": 0,\n",
    "                    **best_params\n",
    "                }\n",
    "                model = xgb.XGBRegressor(**params)\n",
    "\n",
    "                model.fit(\n",
    "                    X_train, y_train,\n",
    "                    eval_set=[(X_valid, y_valid)],\n",
    "                    early_stopping_rounds=100,\n",
    "                    verbose=False\n",
    "                )\n",
    "\n",
    "                valid_predict = model.predict(X_valid)\n",
    "                valid_preds[valid_idx] = valid_predict\n",
    "                preds[valid_idx, 0] += valid_predict / EPOCHS\n",
    "\n",
    "                score = metrics.mean_squared_error(\n",
    "                    y_valid, valid_predict, squared=False)\n",
    "                model_dict[f'Epoch{i + 1}-Fold{fold + 1}'] = model\n",
    "                model.save_model(\n",
    "                    f'./baseline_xgb_models_optuna/xgb_model_epoch{i + 1}_fold{fold + 1}.json')\n",
    "\n",
    "            final_score = metrics.mean_squared_error(\n",
    "                train_feats[target_col], valid_preds, squared=False)\n",
    "            scores.append(final_score)\n",
    "\n",
    "        print(\"Avg Loss:\", np.mean(scores))\n",
    "        return np.mean(scores)\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=25)\n",
    "\n",
    "    print(\"XGBoost Best trial:\")\n",
    "    trial = study.best_trial\n",
    "    print(f\"Value: {trial.value}\")\n",
    "    print(\"Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    \n",
    "    with open('xgb_best_params.json', 'w') as json_file:\n",
    "        json.dump(trial.params, json_file, indent=4)\n",
    "\n",
    "    print(\"Save XGBoost best_params to json file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17468524",
   "metadata": {
    "_kg_hide-output": false,
    "execution": {
     "iopub.execute_input": "2024-01-09T10:07:11.604343Z",
     "iopub.status.busy": "2024-01-09T10:07:11.603947Z",
     "iopub.status.idle": "2024-01-09T10:07:11.620147Z",
     "shell.execute_reply": "2024-01-09T10:07:11.619048Z"
    },
    "papermill": {
     "duration": 0.026746,
     "end_time": "2024-01-09T10:07:11.622231",
     "exception": false,
     "start_time": "2024-01-09T10:07:11.595485",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_cb_optuna(train_feats):\n",
    "    os.makedirs('./baseline_cb_models_optuna', exist_ok=True)\n",
    "\n",
    "    def objective(trial):\n",
    "        EPOCHS = 1\n",
    "        SPLIT = 10\n",
    "\n",
    "        model_dict = {}\n",
    "        scores = []\n",
    "        preds = np.zeros((len(train_feats), 1))\n",
    "\n",
    "        best_params = {\n",
    "            'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-3, 10.0),\n",
    "            'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.1, 1.0),\n",
    "            'subsample': trial.suggest_float('subsample', 0.1, 1.0),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.0005, 1e-1, log=True),\n",
    "            'depth': trial.suggest_int('depth', 1, 15),\n",
    "            'iterations': trial.suggest_int('iterations', 1000, 15000),\n",
    "            'min_child_samples': trial.suggest_int('min_child_samples', 1, 20),\n",
    "            'thread_count': 4\n",
    "        }\n",
    "\n",
    "        for i in range(EPOCHS):\n",
    "            kf = model_selection.KFold(\n",
    "                n_splits=SPLIT, random_state=42 + i * 10, shuffle=True)\n",
    "            valid_preds = np.zeros(train_feats.shape[0])\n",
    "\n",
    "            for fold, (train_idx, valid_idx) in enumerate(kf.split(train_feats)):\n",
    "                print(f'Epoch: {i + 1} Fold: {fold + 1}')\n",
    "                X_train, y_train = train_feats.iloc[train_idx][train_cols], train_feats.iloc[train_idx][target_col]\n",
    "                X_valid, y_valid = train_feats.iloc[valid_idx][train_cols], train_feats.iloc[valid_idx][target_col]\n",
    "\n",
    "                model = cb.CatBoostRegressor(\n",
    "                    loss_function='RMSE',\n",
    "                    random_seed=2023,\n",
    "                    verbose=False,\n",
    "                    **best_params\n",
    "                )\n",
    "\n",
    "                model.fit(\n",
    "                    X_train, y_train,\n",
    "                    eval_set=[(X_valid, y_valid)],\n",
    "                    early_stopping_rounds=100,\n",
    "                    verbose=False\n",
    "                )\n",
    "\n",
    "                valid_predict = model.predict(X_valid)\n",
    "                valid_preds[valid_idx] = valid_predict\n",
    "                preds[valid_idx, 0] += valid_predict / EPOCHS\n",
    "\n",
    "                score = metrics.mean_squared_error(\n",
    "                    y_valid, valid_predict, squared=False)\n",
    "                model_dict[f'Epoch{i + 1}-Fold{fold + 1}'] = model\n",
    "                model.save_model(\n",
    "                    f'./baseline_cb_models_optuna/cb_model_epoch{i + 1}_fold{fold + 1}.cbm')\n",
    "\n",
    "            final_score = metrics.mean_squared_error(\n",
    "                train_feats[target_col], valid_preds, squared=False)\n",
    "            scores.append(final_score)\n",
    "\n",
    "        print(\"Avg Loss:\", np.mean(scores))\n",
    "        return np.mean(scores)\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=100)\n",
    "\n",
    "    print(\"CatBoost Best trial:\")\n",
    "    trial = study.best_trial\n",
    "    print(f\"Value: {trial.value}\")\n",
    "    print(\"Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "        \n",
    "    with open('cb_best_params.json', 'w') as json_file:\n",
    "        json.dump(trial.params, json_file, indent=4)\n",
    "\n",
    "    print(\"Save catboost best_params to json file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb6ccc5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T10:07:11.637669Z",
     "iopub.status.busy": "2024-01-09T10:07:11.636941Z",
     "iopub.status.idle": "2024-01-09T10:29:56.436661Z",
     "shell.execute_reply": "2024-01-09T10:29:56.435723Z"
    },
    "papermill": {
     "duration": 1364.822603,
     "end_time": "2024-01-09T10:29:56.451699",
     "exception": false,
     "start_time": "2024-01-09T10:07:11.629096",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LightGBM Model ..........\n",
      "Epoch: 1 Fold: 1\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003025 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 26601\n",
      "[LightGBM] [Info] Number of data points in the train set: 2223, number of used features: 161\n",
      "[LightGBM] [Info] Start training from score 3.706928\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[291]\tvalid_0's rmse: 0.554164\n",
      "Evaluated only: rmse\n",
      "Epoch: 1 Fold: 2\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003059 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 26620\n",
      "[LightGBM] [Info] Number of data points in the train set: 2224, number of used features: 161\n",
      "[LightGBM] [Info] Start training from score 3.713579\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[417]\tvalid_0's rmse: 0.508669\n",
      "Evaluated only: rmse\n",
      "Epoch: 1 Fold: 3\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003005 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 26573\n",
      "[LightGBM] [Info] Number of data points in the train set: 2224, number of used features: 161\n",
      "[LightGBM] [Info] Start training from score 3.715378\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[839]\tvalid_0's rmse: 0.67181\n",
      "Evaluated only: rmse\n",
      "Epoch: 1 Fold: 4\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002749 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 26583\n",
      "[LightGBM] [Info] Number of data points in the train set: 2224, number of used features: 161\n",
      "[LightGBM] [Info] Start training from score 3.709982\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[301]\tvalid_0's rmse: 0.606673\n",
      "Evaluated only: rmse\n",
      "Epoch: 1 Fold: 5\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002733 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 26616\n",
      "[LightGBM] [Info] Number of data points in the train set: 2224, number of used features: 161\n",
      "[LightGBM] [Info] Start training from score 3.705036\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[286]\tvalid_0's rmse: 0.601648\n",
      "Evaluated only: rmse\n",
      "Epoch: 1 Fold: 6\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002734 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 26585\n",
      "[LightGBM] [Info] Number of data points in the train set: 2224, number of used features: 161\n",
      "[LightGBM] [Info] Start training from score 3.707284\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[344]\tvalid_0's rmse: 0.596103\n",
      "Evaluated only: rmse\n",
      "Epoch: 1 Fold: 7\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002787 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 26612\n",
      "[LightGBM] [Info] Number of data points in the train set: 2224, number of used features: 161\n",
      "[LightGBM] [Info] Start training from score 3.717176\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[257]\tvalid_0's rmse: 0.642997\n",
      "Evaluated only: rmse\n",
      "Epoch: 1 Fold: 8\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002713 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 26622\n",
      "[LightGBM] [Info] Number of data points in the train set: 2224, number of used features: 161\n",
      "[LightGBM] [Info] Start training from score 3.702788\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[170]\tvalid_0's rmse: 0.637469\n",
      "Evaluated only: rmse\n",
      "Epoch: 1 Fold: 9\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002868 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 26603\n",
      "[LightGBM] [Info] Number of data points in the train set: 2224, number of used features: 161\n",
      "[LightGBM] [Info] Start training from score 3.724371\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[323]\tvalid_0's rmse: 0.632459\n",
      "Evaluated only: rmse\n",
      "Epoch: 1 Fold: 10\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002905 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 26604\n",
      "[LightGBM] [Info] Number of data points in the train set: 2224, number of used features: 161\n",
      "[LightGBM] [Info] Start training from score 3.709982\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[168]\tvalid_0's rmse: 0.579621\n",
      "Evaluated only: rmse\n",
      "Epoch: 2 Fold: 1\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002794 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 26604\n",
      "[LightGBM] [Info] Number of data points in the train set: 2223, number of used features: 161\n",
      "[LightGBM] [Info] Start training from score 3.719073\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[381]\tvalid_0's rmse: 0.552216\n",
      "Evaluated only: rmse\n",
      "Epoch: 2 Fold: 2\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002675 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 26590\n",
      "[LightGBM] [Info] Number of data points in the train set: 2224, number of used features: 161\n",
      "[LightGBM] [Info] Start training from score 3.712680\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[356]\tvalid_0's rmse: 0.602634\n",
      "Evaluated only: rmse\n",
      "Epoch: 2 Fold: 3\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002742 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 26587\n",
      "[LightGBM] [Info] Number of data points in the train set: 2224, number of used features: 161\n",
      "[LightGBM] [Info] Start training from score 3.709308\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[274]\tvalid_0's rmse: 0.650393\n",
      "Evaluated only: rmse\n",
      "Epoch: 2 Fold: 4\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002804 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 26601\n",
      "[LightGBM] [Info] Number of data points in the train set: 2224, number of used features: 161\n",
      "[LightGBM] [Info] Start training from score 3.699191\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[366]\tvalid_0's rmse: 0.62571\n",
      "Evaluated only: rmse\n",
      "Epoch: 2 Fold: 5\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002839 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 26623\n",
      "[LightGBM] [Info] Number of data points in the train set: 2224, number of used features: 161\n",
      "[LightGBM] [Info] Start training from score 3.716727\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[649]\tvalid_0's rmse: 0.650205\n",
      "Evaluated only: rmse\n",
      "Epoch: 2 Fold: 6\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002934 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 26589\n",
      "[LightGBM] [Info] Number of data points in the train set: 2224, number of used features: 161\n",
      "[LightGBM] [Info] Start training from score 3.715603\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[222]\tvalid_0's rmse: 0.64791\n",
      "Evaluated only: rmse\n",
      "Epoch: 2 Fold: 7\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004685 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 26611\n",
      "[LightGBM] [Info] Number of data points in the train set: 2224, number of used features: 161\n",
      "[LightGBM] [Info] Start training from score 3.706385\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[223]\tvalid_0's rmse: 0.562567\n",
      "Evaluated only: rmse\n",
      "Epoch: 2 Fold: 8\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002920 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 26622\n",
      "[LightGBM] [Info] Number of data points in the train set: 2224, number of used features: 161\n",
      "[LightGBM] [Info] Start training from score 3.715827\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[201]\tvalid_0's rmse: 0.556852\n",
      "Evaluated only: rmse\n",
      "Epoch: 2 Fold: 9\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003134 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 26594\n",
      "[LightGBM] [Info] Number of data points in the train set: 2224, number of used features: 161\n",
      "[LightGBM] [Info] Start training from score 3.712905\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[216]\tvalid_0's rmse: 0.595744\n",
      "Evaluated only: rmse\n",
      "Epoch: 2 Fold: 10\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002989 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 26606\n",
      "[LightGBM] [Info] Number of data points in the train set: 2224, number of used features: 161\n",
      "[LightGBM] [Info] Start training from score 3.704811\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[250]\tvalid_0's rmse: 0.622534\n",
      "Evaluated only: rmse\n",
      "Epoch: 3 Fold: 1\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002700 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 26588\n",
      "[LightGBM] [Info] Number of data points in the train set: 2223, number of used features: 161\n",
      "[LightGBM] [Info] Start training from score 3.708052\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[315]\tvalid_0's rmse: 0.583239\n",
      "Evaluated only: rmse\n",
      "Epoch: 3 Fold: 2\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002810 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 26607\n",
      "[LightGBM] [Info] Number of data points in the train set: 2224, number of used features: 161\n",
      "[LightGBM] [Info] Start training from score 3.707734\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[239]\tvalid_0's rmse: 0.566225\n",
      "Evaluated only: rmse\n",
      "Epoch: 3 Fold: 3\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002853 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 26581\n",
      "[LightGBM] [Info] Number of data points in the train set: 2224, number of used features: 161\n",
      "[LightGBM] [Info] Start training from score 3.708408\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[259]\tvalid_0's rmse: 0.657676\n",
      "Evaluated only: rmse\n",
      "Epoch: 3 Fold: 4\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002742 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 26633\n",
      "[LightGBM] [Info] Number of data points in the train set: 2224, number of used features: 161\n",
      "[LightGBM] [Info] Start training from score 3.709308\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[400]\tvalid_0's rmse: 0.640881\n",
      "Evaluated only: rmse\n",
      "Epoch: 3 Fold: 5\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002751 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 26618\n",
      "[LightGBM] [Info] Number of data points in the train set: 2224, number of used features: 161\n",
      "[LightGBM] [Info] Start training from score 3.723471\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[191]\tvalid_0's rmse: 0.658949\n",
      "Evaluated only: rmse\n",
      "Epoch: 3 Fold: 6\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002896 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 26581\n",
      "[LightGBM] [Info] Number of data points in the train set: 2224, number of used features: 161\n",
      "[LightGBM] [Info] Start training from score 3.703462\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[279]\tvalid_0's rmse: 0.597423\n",
      "Evaluated only: rmse\n",
      "Epoch: 3 Fold: 7\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002805 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 26595\n",
      "[LightGBM] [Info] Number of data points in the train set: 2224, number of used features: 161\n",
      "[LightGBM] [Info] Start training from score 3.716502\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[424]\tvalid_0's rmse: 0.590488\n",
      "Evaluated only: rmse\n",
      "Epoch: 3 Fold: 8\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003210 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 26623\n",
      "[LightGBM] [Info] Number of data points in the train set: 2224, number of used features: 161\n",
      "[LightGBM] [Info] Start training from score 3.712905\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[270]\tvalid_0's rmse: 0.601505\n",
      "Evaluated only: rmse\n",
      "Epoch: 3 Fold: 9\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002920 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 26603\n",
      "[LightGBM] [Info] Number of data points in the train set: 2224, number of used features: 161\n",
      "[LightGBM] [Info] Start training from score 3.703912\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[192]\tvalid_0's rmse: 0.607874\n",
      "Evaluated only: rmse\n",
      "Epoch: 3 Fold: 10\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002917 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 26577\n",
      "[LightGBM] [Info] Number of data points in the train set: 2224, number of used features: 161\n",
      "[LightGBM] [Info] Start training from score 3.718750\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[130]\tvalid_0's rmse: 0.594071\n",
      "Evaluated only: rmse\n",
      "Avg Loss: 0.607723205030709\n",
      "metric LGBM = 0.60489\n",
      "Training XGBoost Model ..........\n",
      "Epoch: 1 Fold: 1\n",
      "Epoch: 1 Fold: 2\n",
      "Epoch: 1 Fold: 3\n",
      "Epoch: 1 Fold: 4\n",
      "Epoch: 1 Fold: 5\n",
      "Epoch: 1 Fold: 6\n",
      "Epoch: 1 Fold: 7\n",
      "Epoch: 1 Fold: 8\n",
      "Epoch: 1 Fold: 9\n",
      "Epoch: 1 Fold: 10\n",
      "Epoch: 2 Fold: 1\n",
      "Epoch: 2 Fold: 2\n",
      "Epoch: 2 Fold: 3\n",
      "Epoch: 2 Fold: 4\n",
      "Epoch: 2 Fold: 5\n",
      "Epoch: 2 Fold: 6\n",
      "Epoch: 2 Fold: 7\n",
      "Epoch: 2 Fold: 8\n",
      "Epoch: 2 Fold: 9\n",
      "Epoch: 2 Fold: 10\n",
      "Epoch: 3 Fold: 1\n",
      "Epoch: 3 Fold: 2\n",
      "Epoch: 3 Fold: 3\n",
      "Epoch: 3 Fold: 4\n",
      "Epoch: 3 Fold: 5\n",
      "Epoch: 3 Fold: 6\n",
      "Epoch: 3 Fold: 7\n",
      "Epoch: 3 Fold: 8\n",
      "Epoch: 3 Fold: 9\n",
      "Epoch: 3 Fold: 10\n",
      "Avg Loss: 0.6069586443144424\n",
      "metric XGB = 0.60588\n",
      "Training CatBoost Model ..........\n",
      "Epoch: 1 Fold: 1\n",
      "Epoch: 1 Fold: 2\n",
      "Epoch: 1 Fold: 3\n",
      "Epoch: 1 Fold: 4\n",
      "Epoch: 1 Fold: 5\n",
      "Epoch: 1 Fold: 6\n",
      "Epoch: 1 Fold: 7\n",
      "Epoch: 1 Fold: 8\n",
      "Epoch: 1 Fold: 9\n",
      "Epoch: 1 Fold: 10\n",
      "Epoch: 2 Fold: 1\n",
      "Epoch: 2 Fold: 2\n",
      "Epoch: 2 Fold: 3\n",
      "Epoch: 2 Fold: 4\n",
      "Epoch: 2 Fold: 5\n",
      "Epoch: 2 Fold: 6\n",
      "Epoch: 2 Fold: 7\n",
      "Epoch: 2 Fold: 8\n",
      "Epoch: 2 Fold: 9\n",
      "Epoch: 2 Fold: 10\n",
      "Epoch: 3 Fold: 1\n",
      "Epoch: 3 Fold: 2\n",
      "Epoch: 3 Fold: 3\n",
      "Epoch: 3 Fold: 4\n",
      "Epoch: 3 Fold: 5\n",
      "Epoch: 3 Fold: 6\n",
      "Epoch: 3 Fold: 7\n",
      "Epoch: 3 Fold: 8\n",
      "Epoch: 3 Fold: 9\n",
      "Epoch: 3 Fold: 10\n",
      "Avg Loss: 0.6048549204432557\n",
      "metric CB = 0.60278\n"
     ]
    }
   ],
   "source": [
    "if CFG.is_train_lgbm_optuna:\n",
    "    train_lgbm_optuna(train_feats=data)\n",
    "\n",
    "if CFG.is_train_xgb_optuna:\n",
    "    train_xgb_optuna(train_feats=data)\n",
    "\n",
    "if CFG.is_train_cb_optuna:\n",
    "    train_cb_optuna(train_feats=data)\n",
    "\n",
    "if CFG.is_train_lgbm_model:\n",
    "    lgbm_preds = train_lgbm_model(train_feats=data, test_feats=test_feats)\n",
    "\n",
    "if CFG.is_train_xgb_model:\n",
    "    xgb_preds = train_xgb_model(train_feats=data, test_feats=test_feats)\n",
    "\n",
    "if CFG.is_train_cb_model:\n",
    "    cb_preds = train_cb_model(train_feats=data, test_feats=test_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49b9db28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T10:29:56.480091Z",
     "iopub.status.busy": "2024-01-09T10:29:56.479757Z",
     "iopub.status.idle": "2024-01-09T10:29:56.483926Z",
     "shell.execute_reply": "2024-01-09T10:29:56.483127Z"
    },
    "papermill": {
     "duration": 0.020993,
     "end_time": "2024-01-09T10:29:56.485898",
     "exception": false,
     "start_time": "2024-01-09T10:29:56.464905",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weights = [1/3, 1/3, 1/3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea87f319",
   "metadata": {
    "_kg_hide-output": false,
    "execution": {
     "iopub.execute_input": "2024-01-09T10:29:56.514099Z",
     "iopub.status.busy": "2024-01-09T10:29:56.513358Z",
     "iopub.status.idle": "2024-01-09T10:29:56.531614Z",
     "shell.execute_reply": "2024-01-09T10:29:56.530745Z"
    },
    "papermill": {
     "duration": 0.034562,
     "end_time": "2024-01-09T10:29:56.533522",
     "exception": false,
     "start_time": "2024-01-09T10:29:56.498960",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000aaaa</td>\n",
       "      <td>1.871280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2222bbbb</td>\n",
       "      <td>1.468701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4444cccc</td>\n",
       "      <td>1.480304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id     score\n",
       "0  0000aaaa  1.871280\n",
       "1  2222bbbb  1.468701\n",
       "2  4444cccc  1.480304"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds = np.zeros(test_feats.shape[0])\n",
    "test_preds = lgbm_preds * weights[0] + xgb_preds * weights[1] + cb_preds * weights[2]\n",
    "\n",
    "test_feats['score'] = test_preds\n",
    "submission = test_feats[['id', 'score']]\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "submission.head()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 6678907,
     "sourceId": 59291,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30626,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1454.653681,
   "end_time": "2024-01-09T10:29:57.872119",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-01-09T10:05:43.218438",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
